{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPXFqAQ/fRabgwAtfNVo9ST"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Exploratory Data Analysis in Python**\n","\n","# Course Description\n","\n","So you’ve got some interesting data - where do you begin your analysis? This course will cover the process of exploring and analyzing data, from understanding what’s included in a dataset to incorporating exploration findings into a data science workflow.\n","\n","\n","Using data on unemployment figures and plane ticket prices, you’ll leverage Python to summarize and validate data, calculate, identify and replace missing values, and clean both numerical and categorical values. Throughout the course, you’ll create beautiful Seaborn visualizations to understand variables and their relationships.\n","\n","\n","For example, you’ll examine how alcohol use and student performance are related. Finally, the course will show how exploratory findings feed into data science workflows by creating new features, balancing categorical features, and generating hypotheses from findings.\n","\n","\n","By the end of this course, you’ll have the confidence to perform your own exploratory data analysis (EDA) in Python.You’ll be able to explain your findings visually to others and suggest the next steps for gathering insights from your data!"],"metadata":{"id":"RjoEcKOXEZwj"}},{"cell_type":"markdown","source":["# **Chapter 1: Getting to Know a Dataset**\n","What's the best way to approach a new dataset? Learn to validate and summarize categorical and numerical data and create Seaborn visualizations to communicate your findings."],"metadata":{"id":"ATw-KlyREj4R"}},{"cell_type":"markdown","source":["# **Initial exploration**\n","\n","Functions for initial exploration\n","You are researching unemployment rates worldwide and have been given a new dataset to work with. The data has been saved and loaded for you as a pandas DataFrame called unemployment. You've never seen the data before, so your first task is to use a few pandas functions to learn about this new data.\n","\n","pandas has been imported for you as pd.\n","\n","Instructions 1/3\n","\n","## 1\n","Use a pandas function to print the first five rows of the unemployment DataFrame.\n","\n","\n","## 2\n","Use a pandas function to print a summary of column non-missing values and data types from the unemployment DataFrame.\n","## 3\n","Print the summary statistics (count, mean, standard deviation, min, max, and quartile values) of each numerical column in unemployment.\n"],"metadata":{"id":"ZdcNxs4UEv6i"}},{"cell_type":"code","source":["# Print the first five rows of unemployment\n","print(unemployment.head())"],"metadata":{"id":"m3gumxH5FHK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","      country_code          country_name      continent   2010   2011  ...   2017   2018   2019   2020   2021\n","    0          AFG           Afghanistan           Asia  11.35  11.05  ...  11.18  11.15  11.22  11.71  13.28\n","    1          AGO                Angola         Africa   9.43   7.36  ...   7.41   7.42   7.42   8.33   8.53\n","    2          ALB               Albania         Europe  14.09  13.48  ...  13.62  12.30  11.47  13.33  11.82\n","    3          ARE  United Arab Emirates           Asia   2.48   2.30  ...   2.46   2.35   2.23   3.19   3.36\n","    4          ARG             Argentina  South America   7.71   7.18  ...   8.35   9.22   9.84  11.46  10.90\n","\n","    [5 rows x 15 columns]"],"metadata":{"id":"7RrCKlUOFYx1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print a summary of non-missing values and data types in the unemployment DataFrame\n","print(unemployment.info())"],"metadata":{"id":"TObBSOd4FciW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    <class 'pandas.core.frame.DataFrame'>\n","    RangeIndex: 182 entries, 0 to 181\n","    Data columns (total 15 columns):\n","     #   Column        Non-Null Count  Dtype\n","    ---  ------        --------------  -----\n","     0   country_code  182 non-null    object\n","     1   country_name  182 non-null    object\n","     2   continent     177 non-null    object\n","     3   2010          182 non-null    float64\n","     4   2011          182 non-null    float64\n","     5   2012          182 non-null    float64\n","     6   2013          182 non-null    float64\n","     7   2014          182 non-null    float64\n","     8   2015          182 non-null    float64\n","     9   2016          182 non-null    float64\n","     10  2017          182 non-null    float64\n","     11  2018          182 non-null    float64\n","     12  2019          182 non-null    float64\n","     13  2020          182 non-null    float64\n","     14  2021          182 non-null    float64\n","    dtypes: float64(12), object(3)\n","    memory usage: 21.5+ KB"],"metadata":{"id":"-QOYnkvVGJQR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print summary statistics for numerical columns in unemployment\n","print(unemployment.describe())"],"metadata":{"id":"0_Cg8JGzGMrS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","              2010     2011     2012     2013     2014  ...     2017     2018     2019     2020     2021\n","    count  182.000  182.000  182.000  182.000  182.000  ...  182.000  182.000  182.000  182.000  182.000\n","    mean     8.409    8.315    8.318    8.345    8.180  ...    7.669    7.426    7.244    8.421    8.391\n","    std      6.249    6.267    6.367    6.416    6.284  ...    5.902    5.819    5.697    6.041    6.067\n","    min      0.450    0.320    0.480    0.250    0.200  ...    0.140    0.110    0.100    0.210    0.260\n","    25%      4.015    3.775    3.743    3.692    3.625  ...    3.690    3.625    3.487    4.285    4.335\n","    50%      6.965    6.805    6.690    6.395    6.450  ...    5.650    5.375    5.240    6.695    6.425\n","    75%     10.957   11.045   11.285   11.310   10.695  ...   10.315    9.258    9.445   11.155   10.840\n","    max     32.020   31.380   31.020   29.000   28.030  ...   27.040   26.910   28.470   29.220   33.560\n","\n","    [8 rows x 12 columns]"],"metadata":{"id":"onsTO80ZGP_s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Excellent work—you've now learned that unemployment contains 182 rows of country data including country_code, country_name, continent, and unemployment percentages from 2010 through 2021. If you looked very closely, you might have noticed that a few countries are missing information in the continent column! Let's continue exploring this data in the next exercise."],"metadata":{"id":"a6yv-14oGSbU"}},{"cell_type":"markdown","source":["# **Counting categorical values**\n","Recall from the previous exercise that the unemployment DataFrame contains 182 rows of country data including country_code, country_name, continent, and unemployment percentages from 2010 through 2021.\n","\n","You'd now like to explore the categorical data contained in unemployment to understand the data that it contains related to each continent.\n","\n","The unemployment DataFrame has been loaded for you along with pandas as pd.\n","\n","## Instructions\n","\n","Use a pandas function to count the values associated with each continent in the unemployment DataFrame.\n"],"metadata":{"id":"9uSkMn5ZGWK8"}},{"cell_type":"code","source":["# Count the values associated with each continent in unemployment\n","print(unemployment[\"continent\"].value_counts())"],"metadata":{"id":"ylKQbV9IGdeq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Africa           53\n","    Asia             47\n","    Europe           39\n","    North America    18\n","    South America    12\n","    Oceania           8\n","    Name: continent, dtype: int64"],"metadata":{"id":"nxaeTutiGgZ3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Well done! Did you know that there are 23 countries in North America, which includes countries in the Caribbean and Central America? You may have noticed that North America has 18 data points in the unemployment DataFrame, so we are missing information on a few of the countries from our dataset."],"metadata":{"id":"GX5xfhJ4Gi0n"}},{"cell_type":"markdown","source":["# Global unemployment in 2021\n","It's time to explore some of the numerical data in unemployment! What was typical unemployment in a given year? What was the minimum and maximum unemployment rate, and what did the distribution of the unemployment rates look like across the world? A histogram is a great way to get a sense of the answers to these questions.\n","\n","Your task in this exercise is to create a histogram showing the distribution of global unemployment rates in 2021.\n","\n","The unemployment DataFrame has been loaded for you along with pandas as pd.\n","\n","## Instructions\n","\n","Import the required visualization libraries.\n","Create a histogram of the distribution of 2021 unemployment percentages across all countries in unemployment; show a full percentage point in each bin."],"metadata":{"id":"BSiS0dWiGmUT"}},{"cell_type":"code","source":["# Import the required visualization libraries\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Create a histogram of 2021 unemployment; show a full percent in each bin\n","sns.histplot(data=unemployment, x=\"2021\", binwidth=1)\n","plt.show()"],"metadata":{"id":"YbAOmLfLGtPE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nice work—it looks like 2021 unemployment hovered around 3% to 8% for most countries in the dataset, but a few countries experienced very high unemployment of 20% to 35%."],"metadata":{"id":"u0Grtw92HYYz"}},{"cell_type":"markdown","source":["# **Data validation**\n","\n","# Validating continents\n","Your colleague has informed you that the data on unemployment from countries in Oceania is not reliable, and you'd like to identify and exclude these countries from your unemployment data. The .isin() function can help with that!\n","\n","Your task is to use .isin() to identify countries that are not in Oceania. These countries should return True while countries in Oceania should return False. This will set you up to use the results of .isin() to quickly filter out Oceania countries using Boolean indexing.\n","\n","The unemployment DataFrame is available, and pandas has been imported as pd.\n","\n","# Instructions 1/2\n","\n","Define a Series of Booleans describing whether or not each continent is outside of Oceania; call this Series not_oceania."],"metadata":{"id":"B8k5TW5EIq7d"}},{"cell_type":"code","source":["# Define a Series describing whether each continent is outside of Oceania\n","not_oceania = ~unemployment[\"continent\"].isin([\"Oceania\"])"],"metadata":{"id":"rCtu4Y3ZJMKR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 2/2\n","\n","Use Boolean indexing to print the unemployment DataFrame without any of the data related to countries in Oceania."],"metadata":{"id":"ZR_A-uAqJROs"}},{"cell_type":"code","source":["# Define a Series describing whether each continent is outside of Oceania\n","not_oceania = ~unemployment[\"continent\"].isin([\"Oceania\"])\n","\n","# Print unemployment without records related to countries in Oceania\n","print(unemployment[not_oceania])"],"metadata":{"id":"FgMIye6ZJV-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","        country_code          country_name      continent   2010   2011  ...   2017   2018   2019   2020   2021\n","    0            AFG           Afghanistan           Asia  11.35  11.05  ...  11.18  11.15  11.22  11.71  13.28\n","    1            AGO                Angola         Africa   9.43   7.36  ...   7.41   7.42   7.42   8.33   8.53\n","    2            ALB               Albania         Europe  14.09  13.48  ...  13.62  12.30  11.47  13.33  11.82\n","    3            ARE  United Arab Emirates           Asia   2.48   2.30  ...   2.46   2.35   2.23   3.19   3.36\n","    4            ARG             Argentina  South America   7.71   7.18  ...   8.35   9.22   9.84  11.46  10.90\n","    ..           ...                   ...            ...    ...    ...  ...    ...    ...    ...    ...    ...\n","    175          VNM               Vietnam           Asia   1.11   1.00  ...   1.87   1.16   2.04   2.39   2.17\n","    178          YEM           Yemen, Rep.           Asia  12.83  13.23  ...  13.30  13.15  13.06  13.39  13.57\n","    179          ZAF          South Africa         Africa  24.68  24.64  ...  27.04  26.91  28.47  29.22  33.56\n","    180          ZMB                Zambia         Africa  13.19  10.55  ...  11.63  12.01  12.52  12.85  13.03\n","    181          ZWE              Zimbabwe         Africa   5.21   5.37  ...   4.78   4.80   4.83   5.35   5.17\n","\n","    [174 rows x 15 columns]"],"metadata":{"id":"EIXK4wv7JZB0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Well done! You validated categorical data and used your .isin() validation to then exclude data that you weren't interested in! Filtering out data that you don't need at the start of your EDA process is a great way to organize yourself for the exploration yet to come."],"metadata":{"id":"k7MpMhLuJbaL"}},{"cell_type":"markdown","source":["# Validating range\n","Now it's time to validate our numerical data. We saw in the previous lesson using .describe() that the largest unemployment rate during 2021 was nearly 34 percent, while the lowest was just above zero.\n","\n","Your task in this exercise is to get much more detailed information about the range of unemployment data using Seaborn's boxplot, and you'll also visualize the range of unemployment rates in each continent to understand geographical range differences.\n","\n","unemployment is available, and the following have been imported for you: Seaborn as sns, matplotlib.pyplot as plt, and pandas as pd.\n","\n","## Instructions\n","\n","Print the minimum and maximum unemployment rates, in that order, during 2021.\n","Create a boxplot of 2021 unemployment rates, broken down by continent."],"metadata":{"id":"hsS0tftvJhYT"}},{"cell_type":"code","source":["# Print the minimum and maximum unemployment rates during 2021\n","print(unemployment[\"2021\"].min(), unemployment[\"2021\"].max())\n","\n","# Create a boxplot of 2021 unemployment rates, broken down by continent\n","sns.boxplot(data=unemployment, x=\"2021\", y=\"continent\")\n","plt.show()"],"metadata":{"id":"AytIsxDqJqYs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nice work! Notice how different the ranges in unemployment are between continents. For example, Africa's 50th percentile is lower than that of North America, but the range is much wider."],"metadata":{"id":"RLzm6kYSJ1Y3"}},{"cell_type":"markdown","source":["# **Data summarization**\n","# Summaries with .groupby() and .agg()**bold text**\n","In this exercise, you'll explore the means and standard deviations of the yearly unemployment data. First, you'll find means and standard deviations regardless of the continent to observe worldwide unemployment trends. Then, you'll check unemployment trends broken down by continent.\n","\n","The unemployment DataFrame is available, and pandas has been imported as pd.\n","\n","## Instructions 1/2\n","\n","Print the mean and standard deviation of the unemployment rates for each year."],"metadata":{"id":"s55RdlSiKILI"}},{"cell_type":"code","source":["# Print the mean and standard deviation of rates by year\n","print(unemployment.agg([\"mean\", \"std\"]))"],"metadata":{"id":"M_HjVo3QKZvG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","           2010   2011   2012   2013   2014  ...   2017   2018   2019   2020   2021\n","    mean  8.409  8.315  8.318  8.345  8.180  ...  7.669  7.426  7.244  8.421  8.391\n","    std   6.249  6.267  6.367  6.416  6.284  ...  5.902  5.819  5.697  6.041  6.067\n","\n","    [2 rows x 12 columns]"],"metadata":{"id":"HtBFuYO-KcmB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2/2\n","Print the mean and standard deviation of the unemployment rates for each year, grouped by continent."],"metadata":{"id":"_q4RBstMKgXU"}},{"cell_type":"code","source":["# Print yearly mean and standard deviation grouped by continent\n","print(unemployment.groupby(\"continent\").agg([\"mean\", \"std\"]))"],"metadata":{"id":"AfKiMrfcKni5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","<script.py> output:\n","                     2010           2011           2012  ...   2019    2020           2021\n","                     mean    std    mean    std    mean  ...    std    mean    std    mean    std\n","    continent                                            ...\n","    Africa          9.344  7.411   9.369  7.402   9.241  ...  7.455  10.308  7.928  10.474  8.132\n","    Asia            6.241  5.146   5.942  4.780   5.835  ...  5.254   7.012  5.700   6.906  5.415\n","    Europe         11.008  6.392  10.948  6.540  11.326  ...  4.125   7.471  4.071   7.415  3.948\n","    North America   8.663  5.116   8.563  5.377   8.449  ...  4.770   9.298  4.963   9.155  5.076\n","    Oceania         3.623  2.055   3.647  2.008   4.104  ...  2.369   4.274  2.617   4.280  2.672\n","    South America   6.871  2.807   6.518  2.802   6.411  ...  3.380  10.275  3.411   9.924  3.612\n","\n","    [6 rows x 24 columns]"],"metadata":{"id":"lJmWQOJYKrCY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nicely done! This data is well-summarized, but it's a little long. What if you wanted to focus on a summary for just one year and make it more readable? Give it a go in the next exercise!"],"metadata":{"id":"oVRywmipKtcL"}},{"cell_type":"markdown","source":["# **Named aggregations**\n","You've seen how .groupby() and .agg() can be combined to show summaries across categories. Sometimes, it's helpful to name new columns when aggregating so that it's clear in the code output what aggregations are being applied and where.\n","\n","Your task is to create a DataFrame called continent_summary which shows a row for each continent. The DataFrame columns will contain the mean unemployment rate for each continent in 2021 as well as the standard deviation of the 2021 employment rate. And of course, you'll rename the columns so that their contents are clear!\n","\n","The unemployment DataFrame is available, and pandas has been imported as pd.\n","\n","## Instructions\n","\n","Create a column called mean_rate_2021 which shows the mean 2021 unemployment rate for each continent.\n","Create a column called std_rate_2021 which shows the standard deviation of the 2021 unemployment rate for each continent."],"metadata":{"id":"8yk7FHEZKxIK"}},{"cell_type":"code","source":["continent_summary = unemployment.groupby(\"continent\").agg(\n","    # Create the mean_rate_2021 column\n","    mean_rate_2021=(\"2021\", \"mean\"),\n","    # Create the std_rate_2021 column\n","    std_rate_2021=(\"2021\", \"std\")\n",")\n","print(continent_summary)"],"metadata":{"id":"tz6RWKRHK37D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","                   mean_rate_2021  std_rate_2021\n","    continent\n","    Africa                 10.474          8.132\n","    Asia                    6.906          5.415\n","    Europe                  7.415          3.948\n","    North America           9.155          5.076\n","    Oceania                 4.280          2.672\n","    South America           9.924          3.612"],"metadata":{"id":"uElNsuVfK7zc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Super summarizing! Average 2021 unemployment varied widely by continent, and so did the unemployment within those continents."],"metadata":{"id":"HHM2hPfsLA97"}},{"cell_type":"markdown","source":["# Visualizing categorical summaries\n","As you've learned in this chapter, Seaborn has many great visualizations for exploration, including a bar plot for displaying an aggregated average value by category of data.\n","\n","In Seaborn, bar plots include a vertical bar indicating the 95% confidence interval for the categorical mean. Since confidence intervals are calculated using both the number of values and the variability of those values, they give a helpful indication of how much data can be relied upon.\n","\n","Your task is to create a bar plot to visualize the means and confidence intervals of unemployment rates across the different continents.\n","\n","unemployment is available, and the following have been imported for you: Seaborn as sns, matplotlib.pyplot as plt, and pandas as pd.\n","\n","## Instructions\n","\n","Create a bar plot showing continents on the x-axis and their respective average 2021 unemployment rates on the y-axis."],"metadata":{"id":"yAaBoKfdLEl4"}},{"cell_type":"code","source":["# Create a bar plot of continents and their 2021 average unemployment\n","sns.barplot(data=unemployment, x=\"continent\", y=\"2021\")\n","plt.show()"],"metadata":{"id":"Q1eFyCd8LKbz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A perfect plot! Way to go. While Europe has higher average unemployment than Asia, it also has a smaller confidence interval for that average, so the average value is more reliable."],"metadata":{"id":"lWA3N-JhLdef"}},{"cell_type":"markdown","source":["# **Chapter 1: Data Cleaning and Imputation**\n","\n","Exploring and analyzing data often means dealing with missing values, incorrect data types, and outliers. In this chapter, you’ll learn techniques to handle these issues and streamline your EDA processes!"],"metadata":{"id":"6ZKFAh4hLhrX"}},{"cell_type":"markdown","source":["# **Addressing missing data**\n","# Dealing with missing data\n","It is important to deal with missing data before starting your analysis.\n","\n","One approach is to drop missing values if they account for a small proportion, typically five percent, of your data.\n","\n","Working with a dataset on plane ticket prices, stored as a pandas DataFrame called planes, you'll need to count the number of missing values across all columns, calculate five percent of all values, use this threshold to remove observations, and check how many missing values remain in the dataset.\n","\n","## Instructions 1/3\n","\n","Print the number of missing values in each column of the DataFrame."],"metadata":{"id":"HA2SWyxaLxmK"}},{"cell_type":"code","source":["# Count the number of missing values in each column\n","print(planes.isna().sum())"],"metadata":{"id":"4araORmnL98G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Airline            427\n","    Date_of_Journey    322\n","    Source             187\n","    Destination        347\n","    Route              256\n","    Dep_Time           260\n","    Arrival_Time       194\n","    Duration           214\n","    Total_Stops        212\n","    Additional_Info    589\n","    Price              616\n","    dtype: int64"],"metadata":{"id":"cVg1JI7MMCME"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 2/3\n","\n","Calculate how many observations five percent of the planes DataFrame is equal to."],"metadata":{"id":"NQ-_O5E7MGNd"}},{"cell_type":"code","source":["# Count the number of missing values in each column\n","print(planes.isna().sum())\n","\n","# Find the five percent threshold\n","threshold = len(planes) * 0.05"],"metadata":{"id":"JmJbWTnQMLa4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Airline            427\n","    Date_of_Journey    322\n","    Source             187\n","    Destination        347\n","    Route              256\n","    Dep_Time           260\n","    Arrival_Time       194\n","    Duration           214\n","    Total_Stops        212\n","    Additional_Info    589\n","    Price              616\n","    dtype: int64"],"metadata":{"id":"E2BfDBqwMTBs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 3/3\n","\n","Create cols_to_drop by applying boolean indexing to columns of the DataFrame with missing values less than or equal to the threshold.\n","Use this filter to remove missing values and save the updated DataFrame."],"metadata":{"id":"WxHGzXOIMWSf"}},{"cell_type":"code","source":["# Count the number of missing values in each column\n","print(planes.isna().sum())\n","\n","# Find the five percent threshold\n","threshold = len(planes) * 0.05\n","\n","# Create a filter\n","cols_to_drop = planes.columns[planes.isna().sum() <= threshold]\n","\n","# Drop missing values for columns below the threshold\n","planes.dropna(subset=cols_to_drop, inplace=True)\n","\n","print(planes.isna().sum())"],"metadata":{"id":"i2qWl9a2Mbmm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Airline            427\n","    Date_of_Journey    322\n","    Source             187\n","    Destination        347\n","    Route              256\n","    Dep_Time           260\n","    Arrival_Time       194\n","    Duration           214\n","    Total_Stops        212\n","    Additional_Info    589\n","    Price              616\n","    dtype: int64\n","\n","<script.py> output:\n","    Airline            427\n","    Date_of_Journey    322\n","    Source             187\n","    Destination        347\n","    Route              256\n","    Dep_Time           260\n","    Arrival_Time       194\n","    Duration           214\n","    Total_Stops        212\n","    Additional_Info    589\n","    Price              616\n","    dtype: int64\n","    Airline              0\n","    Date_of_Journey      0\n","    Source               0\n","    Destination          0\n","    Route                0\n","    Dep_Time             0\n","    Arrival_Time         0\n","    Duration             0\n","    Total_Stops          0\n","    Additional_Info    300\n","    Price              368\n","    dtype: int64"],"metadata":{"id":"JTYm-hvKMiQv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Awesome! By creating a missing values threshold and using it to filter columns, you've managed to remove missing values from all columns except for \"Additional_Info\" and \"Price\""],"metadata":{"id":"8xc9FCmaMlNv"}},{"cell_type":"markdown","source":["# Strategies for remaining missing data\n","The five percent rule has worked nicely for your planes dataset, eliminating missing values from nine out of 11 columns!\n","\n","Now, you need to decide what to do with the \"Additional_Info\" and \"Price\" columns, which are missing 300 and 368 values respectively.\n","\n","You'll first take a look at what \"Additional_Info\" contains, then visualize the price of plane tickets by different airlines.\n","\n","The following imports have been made for you:\n","\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","## Instructions 1/3\n","\n","Print the values and frequencies of \"Additional_Info\"."],"metadata":{"id":"PsskacumMorL"}},{"cell_type":"code","source":["# Check the values of the Additional_Info column\n","print(planes[\"Additional_Info\"].value_counts())"],"metadata":{"id":"wkm9jxeVMvzm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    No info                         6399\n","    In-flight meal not included     1525\n","    No check-in baggage included     258\n","    1 Long layover                    14\n","    Change airports                    7\n","    No Info                            2\n","    Business class                     1\n","    Red-eye flight                     1\n","    2 Long layover                     1\n","    Name: Additional_Info, dtype: int64"],"metadata":{"id":"dn147oHfMzbP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 2/3\n","\n","Calculate how many observations five percent of the planes DataFrame is equal to."],"metadata":{"id":"rXwHMGOGNPru"}},{"cell_type":"code","source":["# Count the number of missing values in each column\n","print(planes.isna().sum())\n","\n","# Find the five percent threshold\n","threshold = len(planes) * 0.05"],"metadata":{"id":"zQ5KGcm8NV5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Airline            427\n","    Date_of_Journey    322\n","    Source             187\n","    Destination        347\n","    Route              256\n","    Dep_Time           260\n","    Arrival_Time       194\n","    Duration           214\n","    Total_Stops        212\n","    Additional_Info    589\n","    Price              616\n","    dtype: int64"],"metadata":{"id":"oDKw_SUXNaSq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 3/3\n","Create cols_to_drop by applying boolean indexing to columns of the DataFrame with missing values less than or equal to the threshold.\n","Use this filter to remove missing values and save the updated DataFrame."],"metadata":{"id":"4ruUVyn-NeNI"}},{"cell_type":"code","source":["# Count the number of missing values in each column\n","print(planes.isna().sum())\n","\n","# Find the five percent threshold\n","threshold = len(planes) * 0.05\n","\n","# Create a filter\n","cols_to_drop = planes.columns[planes.isna().sum() <= threshold]\n","\n","# Drop missing values for columns below the threshold\n","planes.dropna(subset=cols_to_drop, inplace=True)\n","\n","print(planes.isna().sum())"],"metadata":{"id":"QtLr5RY4NioI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Airline            427\n","    Date_of_Journey    322\n","    Source             187\n","    Destination        347\n","    Route              256\n","    Dep_Time           260\n","    Arrival_Time       194\n","    Duration           214\n","    Total_Stops        212\n","    Additional_Info    589\n","    Price              616\n","    dtype: int64\n","\n","<script.py> output:\n","    Airline            427\n","    Date_of_Journey    322\n","    Source             187\n","    Destination        347\n","    Route              256\n","    Dep_Time           260\n","    Arrival_Time       194\n","    Duration           214\n","    Total_Stops        212\n","    Additional_Info    589\n","    Price              616\n","    dtype: int64\n","    Airline              0\n","    Date_of_Journey      0\n","    Source               0\n","    Destination          0\n","    Route                0\n","    Dep_Time             0\n","    Arrival_Time         0\n","    Duration             0\n","    Total_Stops          0\n","    Additional_Info    300\n","    Price              368\n","    dtype: int64"],"metadata":{"id":"ftL5T_5RNmHZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Awesome! By creating a missing values threshold and using it to filter columns, you've managed to remove missing values from all columns except for \"Additional_Info\" and \"Price\"."],"metadata":{"id":"t7_4568tNodE"}},{"cell_type":"markdown","source":["# Strategies for remaining missing data\n","The five percent rule has worked nicely for your planes dataset, eliminating missing values from nine out of 11 columns!\n","\n","Now, you need to decide what to do with the \"Additional_Info\" and \"Price\" columns, which are missing 300 and 368 values respectively.\n","\n","You'll first take a look at what \"Additional_Info\" contains, then visualize the price of plane tickets by different airlines.\n","\n","The following imports have been made for you:\n","\n","import pandas as pd\n","\n","import seaborn as sns\n","\n","import matplotlib.pyplot as plt\n","## Instructions 1/3\n","\n","Print the values and frequencies of \"Additional_Info\"."],"metadata":{"id":"1wxzC9tkR3tp"}},{"cell_type":"code","source":["# Check the values of the Additional_Info column\n","print(planes[\"Additional_Info\"].value_counts())"],"metadata":{"id":"JaGuXmupSJUA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    No info                         6399\n","    In-flight meal not included     1525\n","    No check-in baggage included     258\n","    1 Long layover                    14\n","    Change airports                    7\n","    No Info                            2\n","    Business class                     1\n","    Red-eye flight                     1\n","    2 Long layover                     1\n","    Name: Additional_Info, dtype: int64"],"metadata":{"id":"cOg_2JFhSMMx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 2/3\n","\n","Create a boxplot of \"Price\" by \"Airline\"."],"metadata":{"id":"BrmOv9UOSlJW"}},{"cell_type":"code","source":["# Check the values of the Additional_Info column\n","print(planes[\"Additional_Info\"].value_counts())\n","\n","# Create a box plot of Price by Airline\n","sns.boxplot(data=planes, x=\"Airline\", y=\"Price\")\n","\n","plt.show()"],"metadata":{"id":"ZYrgL651Sr8E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Converting and analyzing categorical data**\n","\n","# Finding the number of unique values\n","You would like to practice some of the categorical data manipulation and analysis skills that you've just seen. To help identify which data could be reformatted to extract value, you are going to find out which non-numeric columns in the planes dataset have a large number of unique values.\n","\n","pandas has been imported for you as pd, and the dataset has been stored as planes.\n","\n","## Instructions\n","\n","Filter planes for columns that are of \"object\" data type.\n","Loop through the columns in the dataset.\n","Add the column iterator to the print statement, then call the function to return the number of unique values in the column."],"metadata":{"id":"c7diJslpTftb"}},{"cell_type":"code","source":["# Filter the DataFrame for object columns\n","non_numeric = planes.select_dtypes(\"object\")\n","\n","# Loop through columns\n","for col in non_numeric.columns:\n","\n","  # Print the number of unique values\n","  print(f\"Number of unique values in {col} column: \", non_numeric[col].nunique())"],"metadata":{"id":"rcAAmsLOVdHx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Number of unique values in Airline column:  8\n","    Number of unique values in Date_of_Journey column:  44\n","    Number of unique values in Source column:  5\n","    Number of unique values in Destination column:  6\n","    Number of unique values in Route column:  122\n","    Number of unique values in Dep_Time column:  218\n","    Number of unique values in Duration column:  362\n","    Number of unique values in Total_Stops column:  5\n","    Number of unique values in Additional_Info column:  9"],"metadata":{"id":"f5n86QUgVgzu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great looping! Interestingly, \"Duration\" is currently an object column whereas it should be a numeric column, and has 362 unique values! Let's find out more about this column."],"metadata":{"id":"kDDNzBZzVksF"}},{"cell_type":"markdown","source":["# Flight duration categories\n","As you saw, there are 362 unique values in the \"Duration\" column of planes. Calling planes[\"Duration\"].head(), we see the following values:\n","\n","0        19h\n","1     5h 25m\n","2     4h 45m\n","3     2h 25m\n","4    15h 30m\n","Name: Duration, dtype: object\n","Looks like this won't be simple to convert to numbers. However, you could categorize flights by duration and examine the frequency of different flight lengths!\n","\n","You'll create a \"Duration_Category\" column in the planes DataFrame. Before you can do this you'll need to create a list of the values you would like to insert into the DataFrame, followed by the existing values that these should be created from.\n","\n","## Instructions 1/2\n","\n","Create a list of categories containing \"Short-haul\", \"Medium\", and \"Long-haul\"."],"metadata":{"id":"DgCCM7tEVpg9"}},{"cell_type":"code","source":["# Create a list of categories\n","flight_categories = [\"Short-haul\", \"Medium\", \"Long-haul\"]"],"metadata":{"id":"ip9caPIDVvae"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 2/2\n","\n","Create short_flights, a string to capture values of \"0h\", \"1h\", \"2h\", \"3h\", or \"4h\" taking care to avoid values such as \"10h\".\n","Create medium_flights to capture any values between five and nine hours.\n","Create long_flights to capture any values from 10 hours to 16 hours inclusive."],"metadata":{"id":"79xMFhIIVzpw"}},{"cell_type":"code","source":["# Create a list of categories\n","flight_categories = [\"Short-haul\", \"Medium\", \"Long-haul\"]\n","\n","# Create short_flights\n","short_flights = \"^0h|^1h|^2h|^3h|^4h\"\n","\n","# Create medium_flights\n","medium_flights = \"^5h|^6h|^7h|^8h|^9h\"\n","\n","# Create long_flights\n","long_flights = \"10h|11h|12h|13h|14h|15h|16h\""],"metadata":{"id":"oY9VeqkHV40q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nicely done! Now you've created your categories and values, it's time to conditionally add the categories into the DataFrame."],"metadata":{"id":"owqu2T2BV8p4"}},{"cell_type":"markdown","source":["# Adding duration categories\n","Now that you've set up the categories and values you want to capture, it's time to build a new column to analyze the frequency of flights by duration!\n","\n","The variables flight_categories, short_flights, medium_flights, and long_flights that you previously created are available to you.\n","\n","Additionally, the following packages have been imported: pandas as pd, numpy as np, seaborn as sns, and matplotlib.pyplot as plt.\n","\n","## Instructions\n","\n","Create conditions, a list containing subsets of planes[\"Duration\"] based on short_flights, medium_flights, and long_flights.\n","Create the \"Duration_Category\" column by calling a function that accepts your conditions list and flight_categories, setting values not found to \"Extreme duration\".\n","Create a plot showing the count of each category."],"metadata":{"id":"P-t0dF8IWAC7"}},{"cell_type":"code","source":["# Create conditions for values in flight_categories to be created\n","conditions = [\n","    (planes[\"Duration\"].str.contains(short_flights)),\n","    (planes[\"Duration\"].str.contains(medium_flights)),\n","    (planes[\"Duration\"].str.contains(long_flights))\n","]\n","\n","# Apply the conditions list to the flight_categories\n","planes[\"Duration_Category\"] = np.select(conditions,\n","                                        flight_categories,\n","                                        default=\"Extreme duration\")\n","\n","# Plot the counts of each category\n","sns.countplot(data=planes, x=\"Duration_Category\")\n","plt.show()"],"metadata":{"id":"4gqe5LWmWFMt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Creative categorical transformation work! It's clear that the majority of flights are short-haul, and virtually none are longer than 16 hours! Now let's take a deep dive into working with numerical data."],"metadata":{"id":"ZrI78DIHWU5G"}},{"cell_type":"markdown","source":["# **Working with numeric data**\n","\n","# Flight duration\n","You would like to analyze the duration of flights, but unfortunately, the \"Duration\" column in the planes DataFrame currently contains string values.\n","\n","You'll need to clean the column and convert it to the correct data type for analysis.\n","\n","## Instructions 1/4\n","\n","Print the first five values of the \"Duration\" column."],"metadata":{"id":"33BzXj1Wk3bk"}},{"cell_type":"code","source":["# Preview the column\n","print(planes[\"Duration\"].head())"],"metadata":{"id":"RjJQoXNPlCjB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    0                  19.0h\n","    1     5.416666666666667h\n","    2                  4.75h\n","    3    2.4166666666666665h\n","    4                  15.5h\n","    Name: Duration, dtype: object"],"metadata":{"id":"SL0H-zzQlF6a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 2/4\n","\n","Remove \"h\" from the column."],"metadata":{"id":"NfUGz-cXlNz4"}},{"cell_type":"code","source":["# Preview the column\n","print(planes[\"Duration\"].head())\n","\n","# Remove the string character\n","planes[\"Duration\"] = planes[\"Duration\"].str.replace(\"h\", \"\")"],"metadata":{"id":"shbOi3_BlSQS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 3/4\n","\n","Convert the column to float data type."],"metadata":{"id":"phiIZMWhlfYE"}},{"cell_type":"code","source":["# Preview the column\n","print(planes[\"Duration\"].head())\n","\n","# Remove the string character\n","planes[\"Duration\"] = planes[\"Duration\"].str.replace(\"h\", \"\")\n","\n","# Convert to float data type\n","planes[\"Duration\"] = planes[\"Duration\"].astype(float)"],"metadata":{"id":"RzI3LeV1lkSK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 4/4\n","\n","Plot a histogram of \"Duration\" values."],"metadata":{"id":"xTyMaQzDlnlX"}},{"cell_type":"code","source":["# Preview the column\n","print(planes[\"Duration\"].head())\n","\n","# Remove the string character\n","planes[\"Duration\"] = planes[\"Duration\"].str.replace(\"h\", \"\")\n","\n","# Convert to float data type\n","planes[\"Duration\"] = planes[\"Duration\"].astype(float)\n","\n","# Plot a histogram\n","sns.histplot(data=planes, x=\"Duration\")\n","plt.show()"],"metadata":{"id":"kSEu-KyzlsKQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creative cleaning skills! Once the data was in the right format, you were able to plot the distribution of 'Duration' and see that the most common flight length is around three hours."],"metadata":{"id":"aoWPCp2Jl1bM"}},{"cell_type":"markdown","source":["# Adding descriptive statistics\n","Now \"Duration\" and \"Price\" both contain numeric values in the planes DataFrame, you would like to calculate summary statistics for them that are conditional on values in other columns.\n","\n","## Instructions 1/3\n","\n","## 1\n","Add a column to planes containing the standard deviation of \"Price\" based on \"Airline\".\n","\n","## 2\n","Calculate the median for \"Duration\" by \"Airline\", storing it as a column called \"airline_median_duration\".\n","## 3\n","Find the mean \"Price\" by \"Destination\", saving it as a column called \"price_destination_mean\"."],"metadata":{"id":"gTwBx5C1mFjl"}},{"cell_type":"code","source":["# Price standard deviation by Airline\n","planes[\"airline_price_st_dev\"] = planes.groupby(\"Airline\")[\"Price\"].transform(lambda x: x.std())\n","\n","print(planes[[\"Airline\", \"airline_price_st_dev\"]].value_counts())"],"metadata":{"id":"FZlMUtOlmQZo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Airline            airline_price_st_dev\n","    Jet Airways        4230.749                3685\n","    IndiGo             2266.754                1981\n","    Air India          3865.872                1686\n","    Multiple carriers  3763.675                1148\n","    SpiceJet           1790.852                 787\n","    Vistara            2864.268                 455\n","    Air Asia           2016.739                 309\n","    GoAir              2790.815                 182\n","    dtype: int64"],"metadata":{"id":"I1-i3dkumXsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Median Duration by Airline\n","planes[\"airline_median_duration\"] = planes.groupby(\"Airline\")[\"Duration\"].transform(lambda x: x.median())\n","\n","print(planes[[\"Airline\",\"airline_median_duration\"]].value_counts())"],"metadata":{"id":"eZsLVgcSma5e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Airline            airline_median_duration\n","    Jet Airways        13.333                     3685\n","    IndiGo             2.917                      1981\n","    Air India          15.917                     1686\n","    Multiple carriers  10.250                     1148\n","    SpiceJet           2.500                       787\n","    Vistara            3.167                       455\n","    Air Asia           2.833                       309\n","    GoAir              5.167                       182\n","    dtype: int64"],"metadata":{"id":"1sWdZymQmd9C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mean Price by Destination\n","planes[\"price_destination_mean\"] = planes.groupby(\"Destination\")[\"Price\"].transform(lambda x: x.mean())\n","\n","print(planes[[\"Destination\",\"price_destination_mean\"]].value_counts())"],"metadata":{"id":"cV8PvflLmi0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Airline            airline_median_duration\n","    Jet Airways        13.333                     3685\n","    IndiGo             2.917                      1981\n","    Air India          15.917                     1686\n","    Multiple carriers  10.250                     1148\n","    SpiceJet           2.500                       787\n","    Vistara            3.167                       455\n","    Air Asia           2.833                       309\n","    GoAir              5.167                       182\n","    dtype: int64\n","\n","<script.py> output:\n","    Destination  price_destination_mean\n","    Cochin       10506.993                 4391\n","    Banglore     9132.225                  2773\n","    Delhi        5157.794                  1219\n","    New Delhi    11738.589                  888\n","    Hyderabad    5025.210                   673\n","    Kolkata      4801.490                   369\n","    dtype: int64"],"metadata":{"id":"6DmjQG2kmm95"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Terrific transforming! Looks like Jet Airways has the largest standard deviation in price, Air India has the largest median duration, and New Delhi, on average, is the most expensive destination. Now let's look at how to handle outliers."],"metadata":{"id":"JZzOuzUrmnzt"}},{"cell_type":"markdown","source":["#** Handling outliers**\n","# Identifying outliers\n","You've proven that you recognize what to do when presented with outliers, but can you identify them using visualizations?\n","\n","Try to figure out if there are outliers in the \"Price\" or \"Duration\" columns of the planes DataFrame.\n","\n","matplotlib.pyplot and seaborn have been imported for you as plt and sns respectively.\n","\n","## Instructions 1/3\n","\n","Plot the distribution of \"Price\" column from planes."],"metadata":{"id":"94CjNzEPmuXC"}},{"cell_type":"code","source":["# Plot a histogram of flight prices\n","sns.histplot(data=planes, x=\"Price\")\n","plt.show()"],"metadata":{"id":"hBtEo1JGm72n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 2/3\n","\n","Display the descriptive statistics for flight duration.\n","\n"],"metadata":{"id":"5P6sr0JHnTrg"}},{"cell_type":"code","source":["# Plot a histogram of flight prices\n","sns.histplot(data=planes, x=\"Price\")\n","plt.show()\n","\n","# Display descriptive statistics for flight duration\n","print(planes[\"Duration\"].describe())"],"metadata":{"id":"_QsRZl_AnYqs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    count    10446.000\n","    mean        10.724\n","    std          8.472\n","    min          0.083\n","    25%          2.833\n","    50%          8.667\n","    75%         15.500\n","    max         47.667\n","    Name: Duration, dtype: float64"],"metadata":{"id":"5ktSV_S3ncM9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Removing outliers\n","While removing outliers isn't always the way to go, for your analysis, you've decided that you will only include flights where the \"Price\" is not an outlier.\n","\n","Therefore, you need to find the upper threshold and then use it to remove values above this from the planes DataFrame.\n","\n","pandas has been imported for you as pd, along with seaborn as sns.\n","\n","## Instructions 1/4\n","\n","Find the 75th and 25th percentiles, saving as price_seventy_fifth and price_twenty_fifth respectively.\n","\n","## 2\n","Calculate the IQR, storing it as prices_iqr.\n","## 3\n","Calculate the upper and lower outlier thresholds.\n","## 4\n","Remove the outliers from planes."],"metadata":{"id":"dqnD4WSEntaQ"}},{"cell_type":"code","source":["# Find the 75th and 25th percentiles\n","price_seventy_fifth = planes[\"Price\"].quantile(0.75)\n","price_twenty_fifth = planes[\"Price\"].quantile(0.25)"],"metadata":{"id":"1h7KsfBWn6Nw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find the 75th and 25th percentiles\n","price_seventy_fifth = planes[\"Price\"].quantile(0.75)\n","price_twenty_fifth = planes[\"Price\"].quantile(0.25)\n","\n","# Calculate iqr\n","prices_iqr = price_seventy_fifth - price_twenty_fifth"],"metadata":{"id":"EVUKh_Vfn_Tg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find the 75th and 25th percentiles\n","price_seventy_fifth = planes[\"Price\"].quantile(0.75)\n","price_twenty_fifth = planes[\"Price\"].quantile(0.25)\n","\n","# Calculate iqr\n","prices_iqr = price_seventy_fifth - price_twenty_fifth\n","\n","# Calculate the thresholds\n","upper = price_seventy_fifth + (1.5 * prices_iqr)\n","lower = price_twenty_fifth - (1.5 * prices_iqr)"],"metadata":{"id":"5GUgDOgFoD7m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find the 75th and 25th percentiles\n","price_seventy_fifth = planes[\"Price\"].quantile(0.75)\n","price_twenty_fifth = planes[\"Price\"].quantile(0.25)\n","\n","# Calculate iqr\n","prices_iqr = price_seventy_fifth - price_twenty_fifth\n","\n","# Calculate the thresholds\n","upper = price_seventy_fifth + (1.5 * prices_iqr)\n","lower = price_twenty_fifth - (1.5 * prices_iqr)\n","\n","# Subset the data\n","planes = planes[(planes[\"Price\"] > lower) & (planes[\"Price\"] < upper)]\n","\n","print(planes[\"Price\"].describe())"],"metadata":{"id":"QfukjdFNoIgm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    count     9959.000\n","    mean      8875.161\n","    std       4057.202\n","    min       1759.000\n","    25%       5228.000\n","    50%       8283.000\n","    75%      12284.000\n","    max      23001.000\n","    Name: Price, dtype: float64"],"metadata":{"id":"HOArmLjCoLa9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ridiculous outlier removal skills! You managed to create thresholds based on the IQR and used them to filter the planes dataset to eliminate extreme prices. Originally the dataset had a maximum price of almost 55000, but the output of planes.describe() shows the maximum has been reduced to around 23000, reflecting a less skewed distribution for analysis!"],"metadata":{"id":"7exdBBzloN06"}},{"cell_type":"markdown","source":["# **Chapter 3: Relationships in Dat**\n","Variables in datasets don't exist in a vacuum; they have relationships with each other. In this chapter, you'll look at relationships across numerical, categorical, and even DateTime data, exploring the direction and strength of these relationships as well as ways to visualize them."],"metadata":{"id":"gmKxxot0oTCB"}},{"cell_type":"markdown","source":["# **Patterns over time**\n","\n","# Importing DateTime data\n","You'll now work with the entire divorce dataset! The data describes Mexican marriages dissolved between 2000 and 2015. It contains marriage and divorce dates, education level, birthday, income for each partner, and marriage duration, as well as the number of children the couple had at the time of divorce.\n","\n","The column names and data types are as follows:\n","\n","divorce_date          object\n","dob_man               object\n","education_man         object\n","income_man           float64\n","dob_woman             object\n","education_woman       object\n","income_woman         float64\n","marriage_date         object\n","marriage_duration    float64\n","num_kids             float64\n","It looks like there is a lot of date information in this data that is not yet a DateTime data type! Your task is to fix that so that you can explore patterns over time.\n","\n","pandas has been imported as pd.\n","\n","## Instructions\n","\n","Import divorce.csv, saving as a DataFrame, divorce; indicate in the import function that the divorce_date, dob_man, dob_woman, and marriage_date columns should be imported as DateTime values."],"metadata":{"id":"N2QsTEM8ohYI"}},{"cell_type":"code","source":["# Import divorce.csv, parsing the appropriate columns as dates in the import\n","divorce = pd.read_csv(\"divorce.csv\", parse_dates=[\"divorce_date\", \"dob_man\", \"dob_woman\", \"marriage_date\"])\n","print(divorce.dtypes)"],"metadata":{"id":"lo2ycmiiosJe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    divorce_date         datetime64[ns]\n","    dob_man              datetime64[ns]\n","    education_man                object\n","    income_man                  float64\n","    dob_woman            datetime64[ns]\n","    education_woman              object\n","    income_woman                float64\n","    marriage_date        datetime64[ns]\n","    marriage_duration           float64\n","    num_kids                    float64\n","    dtype: object"],"metadata":{"id":"ME93tsVmo0Hr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bingo! Nice work parsing those dates at the same time as you imported the data into pandas. Next, have a go at updating DateTime data types in a DataFrame that has already been imported"],"metadata":{"id":"sLfl-lGro39_"}},{"cell_type":"markdown","source":["# Visualizing relationships over time\n","Now that your date data is saved as DateTime data, you can explore patterns over time! Does the year that a couple got married have a relationship with the number of children that the couple has at the time of divorce? Your task is to find out!\n","\n","The divorce DataFrame (with all dates formatted as DateTime data types) has been loaded for you. pandas has been loaded as pd, matplotlib.pyplot has been loaded as plt, and Seaborn has been loaded as sns.\n","\n","## Instructions 1/2\n","\n","Define a column called marriage_year, which contains just the year portion of the marriage_date column."],"metadata":{"id":"V-N2eRsKpCLn"}},{"cell_type":"code","source":["# Define the marriage_year column\n","divorce[\"marriage_year\"] = divorce[\"marriage_date\"].dt.year"],"metadata":{"id":"6UmdhTOCpOxL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 2/2\n","\n","Create a line plot showing the average number of kids a couple had during their marriage, arranged by the year that the couple got married."],"metadata":{"id":"zLH4e44VpUQz"}},{"cell_type":"code","source":["# Define the marriage_year column\n","divorce[\"marriage_year\"] = divorce[\"marriage_date\"].dt.year\n","\n","# Create a line plot showing the average number of kids by year\n","sns.lineplot(data=divorce, x=\"marriage_year\", y=\"num_kids\")\n","plt.show()"],"metadata":{"id":"a1ouUbkCpZKc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nice! You've discovered a pattern here: it looks like couples who had later marriage years also had fewer children during their marriage. We'll explore this relationship and others further in the next video."],"metadata":{"id":"5pRJMAl_pty8"}},{"cell_type":"markdown","source":["#**Correlation**\n","# Visualizing variable relationships\n","In the last exercise, you may have noticed that a longer marriage_duration is correlated with having more children, represented by the num_kids column. The correlation coefficient between the marriage_duration and num_kids variables is 0.45.\n","\n","In this exercise, you'll create a scatter plot to visualize the relationship between these variables. pandas has been loaded as pd, matplotlib.pyplot has been loaded as plt, and Seaborn has been loaded as sns.\n","\n","## Instructions\n","\n","Create a scatterplot showing marriage_duration on the x-axis and num_kids on the y-axis"],"metadata":{"id":"vzjWyAwypzbk"}},{"cell_type":"code","source":["# Create the scatterplot\n","sns.scatterplot(data=divorce, x=\"marriage_duration\", y=\"num_kids\")\n","plt.show()"],"metadata":{"id":"vFmHC7iBqADg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bingo! There is a slight positive relationship in your scatterplot. In the dataset, couples with no children have no value in the num_kids column. If you are confident that all or most of the missing values in num_kids are related to couples without children, you could consider updating these values to 0, which might increase the correlation."],"metadata":{"id":"2w9vuTi1qUNa"}},{"cell_type":"markdown","source":["# Visualizing multiple variable relationships\n","Seaborn's .pairplot() is excellent for understanding the relationships between several or all variables in a dataset by aggregating pairwise scatter plots in one visual.\n","\n","Your task is to use a pairplot to compare the relationship between marriage_duration and income_woman. pandas has been loaded as pd, matplotlib.pyplot has been loaded as plt, and Seaborn has been loaded as sns.\n","\n","## Instructions\n","\n","Create a pairplot to visualize the relationships between income_woman and marriage_duration in the divorce DataFrame."],"metadata":{"id":"b7qu7LzvqkXb"}},{"cell_type":"code","source":["# Create a pairplot for income_woman and marriage_duration\n","sns.pairplot(data=divorce, vars=[\"income_woman\", \"marriage_duration\"])\n","plt.show()"],"metadata":{"id":"RDnsYyLxqp6b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Well done! Just as in the correlation matrix, you can see that the relationship between income_woman and marriage_duration is not a strong one. You can also get a sense of the distributions of both variables in the upper left and lower right plots."],"metadata":{"id":"9Ssn6SYSq_fM"}},{"cell_type":"markdown","source":["# **Factor relationships and distributions**\n","\n","# Categorial data in scatter plots\n","In the video, we explored how men's education and age at marriage related to other variables in our dataset, the divorce DataFrame. Now, you'll take a look at how women's education and age at marriage relate to other variables!\n","\n","Your task is to create a scatter plot of each woman's age and income, layering in the categorical variable of education level for additional context.\n","\n","The divorce DataFrame has been loaded for you, and woman_age_marriage has already been defined as a column representing an estimate of the woman's age at the time of marriage. pandas has been loaded as pd, matplotlib.pyplot has been loaded as plt, and Seaborn has been loaded as sns.\n","\n","## Instructions\n","\n","Create a scatter plot that shows woman_age_marriage on the x-axis and income_woman on the y-axis; each data point should be colored based on the woman's level of education, represented by education_woman.\n"],"metadata":{"id":"wEMcIJC0rFJW"}},{"cell_type":"code","source":["# Create the scatter plot\n","sns.scatterplot(data=divorce, x=\"woman_age_marriage\", y=\"income_woman\", hue=\"education_woman\")\n","plt.show()"],"metadata":{"id":"SA7Nw-kurRkS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Awesome—it looks like there is a positive correlation between professional education and higher salaries, as you might expect. The relationship between women's age at marriage and education level is a little less clear."],"metadata":{"id":"566j7MXqrWJ8"}},{"cell_type":"markdown","source":["# Exploring with KDE plots\n","Kernel Density Estimate (KDE) plots are a great alternative to histograms when you want to show multiple distributions in the same visual.\n","\n","Suppose you are interested in the relationship between marriage duration and the number of kids that a couple has. Since values in the num_kids column range only from one to five, you can plot the KDE for each value on the same plot.\n","\n","The divorce DataFrame has been loaded for you. pandas has been loaded as pd, matplotlib.pyplot has been loaded as plt, and Seaborn has been loaded as sns. Recall that the num_kids column in divorce lists only N/A values for couples with no children, so you'll only be looking at distributions for divorced couples with at least one child.\n","\n","Instructions 1/3\n","\n","Create a KDE plot that shows marriage_duration on the x-axis and a different colored line for each possible number of children that a couple might have, represented by num_kids."],"metadata":{"id":"lresTWIartgD"}},{"cell_type":"code","source":["# Create the KDE plot\n","sns.kdeplot(data=divorce, x=\"marriage_duration\", hue=\"num_kids\")\n","plt.show()"],"metadata":{"id":"xR0OsoTKr016"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 2/3\n","\n","Notice that the plot currently shows marriage durations less than zero; update the KDE plot so that marriage duration cannot be smoothed past the extreme data"],"metadata":{"id":"2KyTdu7qsMU1"}},{"cell_type":"code","source":["# Update the KDE plot so that marriage duration can't be smoothed too far\n","sns.kdeplot(data=divorce, x=\"marriage_duration\", hue=\"num_kids\", cut=0)\n","plt.show()"],"metadata":{"id":"cn1LCQxNsgf8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 3/3\n","\n","Update the code for the KDE plot from the previous step to show a cumulative distribution function for each number of children a couple has."],"metadata":{"id":"vC1anPoYs3SA"}},{"cell_type":"code","source":["# Update the KDE plot to show a cumulative distribution function\n","sns.kdeplot(data=divorce, x=\"marriage_duration\", hue=\"num_kids\", cut=0, cumulative=True)\n","plt.show()"],"metadata":{"id":"YIF7tpZcs8f3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Well done! It looks as though there is a positive correlation between longer marriages and more children, but of course, this doesn't indicate causation. You can also see that there is much less data on couples with more than two children; this helps us understand how reliable our findings are."],"metadata":{"id":"I_mqF1VHtDy9"}},{"cell_type":"markdown","source":["# **Chapter 4: Relationships in Data**\n","\n","Variables in datasets don't exist in a vacuum; they have relationships with each other. In this chapter, you'll look at relationships across numerical, categorical, and even DateTime data, exploring the direction and strength of these relationships as well as ways to visualize them."],"metadata":{"id":"LZRwMn8xwTu3"}},{"cell_type":"markdown","source":["# **Considerations for categorical data**\n","\n","# Checking for class imbalance\n","The 2022 Kaggle Survey captures information about data scientists' backgrounds, preferred technologies, and techniques. It is seen as an accurate view of what is happening in data science based on the volume and profile of responders.\n","\n","Having looked at the job titles and categorized to align with our salaries DataFrame, you can see the following proportion of job categories in the Kaggle survey:\n","\n","Job Category\tRelative Frequency\n","Data Science\t0.281236\n","Data Analytics\t0.224231\n","Other\t0.214609\n","Managerial\t0.121300\n","Machine Learning\t0.083248\n","Data Engineering\t0.075375\n","Thinking of the Kaggle survey results as the population, your task is to find out whether the salaries DataFrame is representative by comparing the relative frequency of job categories.\n","\n","## Instructions\n","\n","Print the relative frequency of the \"Job_Category\" column from salaries DataFrame."],"metadata":{"id":"vNFu6UuhwgMq"}},{"cell_type":"code","source":["# Print the relative frequency of Job_Category\n","print(salaries[\"Job_Category\"].value_counts(normalize=True))"],"metadata":{"id":"r-edIc6dwrbN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Data Science        0.278\n","    Data Engineering    0.273\n","    Data Analytics      0.226\n","    Machine Learning    0.120\n","    Other               0.069\n","    Managerial          0.034\n","    Name: Job_Category, dtype: float64"],"metadata":{"id":"6xllDV5wwuE7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fantastic relative frequency calculation! It looks like Data Science is the most popular class and has a similar representation. Still, the other categories have quite different relative frequencies, which might not be surprising given the target audience is data scientists! Given the difference in relative frequencies, can you trust the salaries DataFrame to accurately represent Managerial roles?"],"metadata":{"id":"43-AH2s-wwZ2"}},{"cell_type":"markdown","source":["# Cross-tabulation\n","Cross-tabulation can help identify how observations occur in combination.\n","\n","Using the salaries dataset, which has been imported as a pandas DataFrame, you'll perform cross-tabulation on multiple variables, including the use of aggregation, to see the relationship between \"Company_Size\" and other variables.\n","\n","pandas has been imported for you as pd.\n","\n","## Instructions 1/3\n","\n","Perform cross-tabulation, setting \"Company_Size\" as the index, and the columns to classes in \"Experience\"."],"metadata":{"id":"ETn-prgAxQTH"}},{"cell_type":"code","source":["# Cross-tabulate Company_Size and Experience\n","print(pd.crosstab(salaries[\"Company_Size\"], salaries[\"Experience\"]))"],"metadata":{"id":"0eSPKTFRxX3S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Experience    EN  EX  MI   SE\n","    Company_Size\n","    L             24   7  49   44\n","    M             25   9  58  136\n","    S             18   1  21   15"],"metadata":{"id":"L45vFUISxb24"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2/3\n","Cross-tabulate \"Job_Category\" and classes of \"Company_Size\" as column names."],"metadata":{"id":"8OVVdeJLxeN_"}},{"cell_type":"code","source":["# Cross-tabulate Job_Category and Company_Size\n","print(pd.crosstab(salaries[\"Job_Category\"], salaries[\"Company_Size\"]))"],"metadata":{"id":"lhhp3mqJxmDb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Company_Size       L   M   S\n","    Job_Category\n","    Data Analytics    23  61   8\n","    Data Engineering  28  72  11\n","    Data Science      38  59  16\n","    Machine Learning  17  19  13\n","    Managerial         5   8   1\n","    Other             13   9   6"],"metadata":{"id":"r2q8YBS6xpsb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3/3\n","Update pd.crosstab() to return the mean \"Salary_USD\" values."],"metadata":{"id":"BP1PzArHxr1r"}},{"cell_type":"code","source":["# Cross-tabulate Job_Category and Company_Size\n","print(pd.crosstab(salaries[\"Job_Category\"], salaries[\"Company_Size\"],\n","            values=salaries[\"Salary_USD\"], aggfunc=\"mean\"))"],"metadata":{"id":"XubQUo-jxyYe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","<script.py> output:\n","    Company_Size       L   M   S\n","    Job_Category\n","    Data Analytics    23  61   8\n","    Data Engineering  28  72  11\n","    Data Science      38  59  16\n","    Machine Learning  17  19  13\n","    Managerial         5   8   1\n","    Other             13   9   6\n","\n","<script.py> output:\n","    Company_Size               L           M          S\n","    Job_Category\n","    Data Analytics    112851.749   95912.685  53741.877\n","    Data Engineering  118939.035  121287.061  86927.136\n","    Data Science       96489.520  116044.456  62241.749\n","    Machine Learning  140779.492  100794.237  78812.586\n","    Managerial        190551.449  150713.628  31484.700\n","    Other              92873.911   89750.579  69871.248"],"metadata":{"id":"dDwaxMSrx3O8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Awesome cross-tabulation! This is a handy function to examine the combination of frequencies, as well as find aggregated statistics. Looks like the largest mean salary is for Managerial data roles in large companies!"],"metadata":{"id":"FbHFXd2Ex6DM"}},{"cell_type":"markdown","source":["# Generating new features\n","\n","# Extracting features for correlation\n","In this exercise, you'll work with a version of the salaries dataset containing a new column called \"date_of_response\".\n","\n","The dataset has been read in as a pandas DataFrame, with \"date_of_response\" as a datetime data type.\n","\n","Your task is to extract datetime attributes from this column and then create a heat map to visualize the correlation coefficients between variables.\n","\n","Seaborn has been imported for you as sns, pandas as pd, and matplotlib.pyplot as plt.\n","\n","## Instructions\n","\n","Extract the month from \"date_of_response\", storing it as a column called \"month\".\n","Create the \"weekday\" column, containing the weekday that the participants completed the survey.\n","Plot a heat map, including the Pearson correlation coefficient scores."],"metadata":{"id":"Zac3Cafbyz7x"}},{"cell_type":"code","source":["# Get the month of the response\n","salaries[\"month\"] = salaries[\"date_of_response\"].dt.month\n","\n","# Extract the weekday of the response\n","salaries[\"weekday\"] = salaries[\"date_of_response\"].dt.weekday\n","\n","# Create a heatmap\n","sns.heatmap(salaries.corr(), annot=True)\n","plt.show()"],"metadata":{"id":"SdxYhCxdzFlG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fantastic feature creation! Looks like there aren't any meaningful relationships between our numeric variables, so let's see if converting numeric data into classes offers additional insights."],"metadata":{"id":"g9rrMqxVzbHF"}},{"cell_type":"markdown","source":["# Calculating salary percentiles\n","In the video, you saw that the conversion of numeric data into categories sometimes makes it easier to identify patterns.\n","\n","Your task is to convert the \"Salary_USD\" column into categories based on its percentiles. First, you need to find the percentiles and store them as variables.\n","\n","pandas has been imported as pd and the salaries dataset read in as DataFrame called salaries.\n","\n","## Instructions\n","\n","Find the 25th percentile of \"Salary_USD\".\n","Store the median of \"Salary_USD\" as salaries_median.\n","Get the 75th percentile of salaries."],"metadata":{"id":"JSSd3cQRzf1f"}},{"cell_type":"code","source":["# Find the 25th percentile\n","twenty_fifth = salaries[\"Salary_USD\"].quantile(0.25)\n","\n","# Save the median\n","salaries_median = salaries[\"Salary_USD\"].median()\n","\n","# Gather the 75th percentile\n","seventy_fifth = salaries[\"Salary_USD\"].quantile(0.75)\n","print(twenty_fifth, salaries_median, seventy_fifth)"],"metadata":{"id":"zB_seFAczmRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    60880.691999999995 97488.552 143225.1"],"metadata":{"id":"Z56yK3f3zptd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Looks like the interquartile range is between 60,881 and 143,225 dollars! Now let's use these variables to add a categorical salary column into the DataFrame!"],"metadata":{"id":"s33VmY69zsok"}},{"cell_type":"markdown","source":["Categorizing salaries\n","Now it's time to make a new category! You'll use the variables twenty_fifth, salaries_median, and seventy_fifth, that you created in the previous exercise, to split salaries into different labels.\n","\n","The result will be a new column called \"salary_level\", which you'll incorporate into a visualization to analyze survey respondents' salary and at companies of different sizes.\n","\n","pandas has been imported as pd, matplotlib.pyplot as plt, seaborn as sns, and the salaries dataset as a pandas DataFrame called salaries.\n","\n","Instructions 1/4\n","\n","Create salary_labels, a list containing \"entry\", \"mid\", \"senior\", and \"exec\"."],"metadata":{"id":"vSFS7pcWz_kF"}},{"cell_type":"code","source":["# Create salary labels\n","salary_labels = [\"entry\", \"mid\", \"senior\", \"exec\"]"],"metadata":{"id":"1WYIUnJH0D_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 2/4\n","\n","Finish salary_ranges, adding the 25th percentile, median, 75th percentile, and largest value from \"Salary_USD\"."],"metadata":{"id":"OpNDyV_o0JfY"}},{"cell_type":"code","source":["# Create salary labels\n","salary_labels = [\"entry\", \"mid\", \"senior\", \"exec\"]\n","\n","# Create the salary ranges list\n","salary_ranges = [0, twenty_fifth, salaries_median, seventy_fifth, salaries[\"Salary_USD\"].max()]"],"metadata":{"id":"4zfwX_h60Ojk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 3/4\n","\n","Split \"Salary_USD\" based on the labels and ranges you've created."],"metadata":{"id":"H2AxFg3T0T0j"}},{"cell_type":"code","source":["# Create salary labels\n","salary_labels = [\"entry\", \"mid\", \"senior\", \"exec\"]\n","\n","# Create the salary ranges list\n","salary_ranges = [0, twenty_fifth, salaries_median, seventy_fifth, salaries[\"Salary_USD\"].max()]\n","\n","# Create salary_level\n","salaries[\"salary_level\"] = pd.cut(salaries[\"Salary_USD\"],\n","                                  bins=salary_ranges,\n","                                  labels=salary_labels)"],"metadata":{"id":"DtzpdFy20cCf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Instructions 4/4\n","\n","Use sns.countplot() to visualize the count of \"Company_Size\", factoring salary level labels."],"metadata":{"id":"8TpDWrln0jOd"}},{"cell_type":"code","source":["# Create salary labels\n","salary_labels = [\"entry\", \"mid\", \"senior\", \"exec\"]\n","\n","# Create the salary ranges list\n","salary_ranges = [0, twenty_fifth, salaries_median, seventy_fifth, salaries[\"Salary_USD\"].max()]\n","\n","# Create salary_level\n","salaries[\"salary_level\"] = pd.cut(salaries[\"Salary_USD\"],\n","                                  bins=salary_ranges,\n","                                  labels=salary_labels)\n","\n","# Plot the count of salary levels at companies of different sizes\n","sns.countplot(data=salaries, x=\"Company_Size\", hue=\"salary_level\")\n","plt.show()"],"metadata":{"id":"BjLB7W1B0oZ6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nice work! By using pd.cut() to split out numeric data into categories, you can see that a large proportion of workers at small companies get paid \"entry\" level salaries, while more staff at medium-sized companies are rewarded with \"senior\" level salary. Now let's look at generating hypotheses as you reach the end of the EDA phase!"],"metadata":{"id":"AUKa9pRX06o9"}},{"cell_type":"markdown","source":["# **Generating hypotheses**\n","\n","# Comparing salaries\n","Exploratory data analysis is a crucial step in generating hypotheses!\n","\n","You've had an idea you'd like to explore—do data professionals get paid more in the USA than they do in Great Britain?\n","\n","You'll need to subset the data by \"Employee_Location\" and produce a plot displaying the average salary between the two groups.\n","\n","The salaries DataFrame has been imported as a pandas DataFrame.\n","\n","pandas has been imported as pd, maplotlib.pyplot as plt and seaborn as sns.\n","\n","## Instructions\n","\n","Filter salaries where \"Employee_Location\" is \"US\" or \"GB\", saving as usa_and_gb.\n","Use usa_and_gb to create a barplot visualizing \"Salary_USD\" against \"Employee_Location\"."],"metadata":{"id":"DJk2Q4_609mV"}},{"cell_type":"code","source":["# Filter for employees in the US or GB\n","usa_and_gb = salaries[salaries[\"Employee_Location\"].isin([\"US\", \"GB\"])]\n","\n","# Create a barplot of salaries by location\n","sns.barplot(data=usa_and_gb, x=\"Employee_Location\", y=\"Salary_USD\")\n","plt.show()"],"metadata":{"id":"X7qFAkfp1Nsv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nicely done! By subsetting the data you were able to directly compare salaries between the USA and Great Britain. The visualization suggests you've generated a hypothesis that is worth formally investigating to determine whether a real difference exists or not!"],"metadata":{"id":"N3JE4TlQ1irT"}},{"cell_type":"markdown","source":["# Choosing a hypothesis\n","You've seen how visualizations can be used to generate hypotheses, making them a crucial part of exploratory data analysis!\n","\n","In this exercise, you'll generate a bar plot to inspect how salaries differ based on company size and employment status. For reference, there are four values:\n","\n","Value\tMeaning\n","CT\tContractor\n","FL\tFreelance\n","PT\tPart-time\n","FT\tFull-time\n","pandas has been imported as pd, matplotlib.pyplot as plt, seaborn as sns, and the salaries dataset as a pandas DataFrame called salaries.\n","\n","## Instructions 1/2\n","\n","Produce a barplot comparing \"Salary_USD\" by \"Company_Size\", factoring \"Employment_Status\"."],"metadata":{"id":"BJYFql2b1nl1"}},{"cell_type":"code","source":["# Create a bar plot of salary versus company size, factoring in employment status\n","sns.barplot(data=salaries, x=\"Company_Size\", y=\"Salary_USD\", hue=\"Employment_Status\")\n","plt.show()"],"metadata":{"id":"y3zpitUR1uS_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[https://drive.google.com/file/d/1Ove5PeR-8TsSy0pVIl9fUZ3q_f4cJ2Lf/view?usp=share_link)"],"metadata":{"id":"175-M97n5Fi1"}}]}