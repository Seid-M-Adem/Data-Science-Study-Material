{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSBs18FwqTbztityUPcIP9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Joining Data with pandas**\n","\n","# Course Description\n","\n","Being able to combine and work with multiple datasets is an essential skill for any aspiring Data Scientist. pandas is a crucial cornerstone of the Python data science ecosystem, with Stack Overflow recording 5 million views for pandas questions. Learn to handle multiple DataFrames by combining, organizing, joining, and reshaping them using pandas. You'll work with datasets from the World Bank and the City Of Chicago. You will finish the course with a solid skillset for data-joining in pandas."],"metadata":{"id":"OLsz2DkC0nqS"}},{"cell_type":"markdown","source":["# Chapter 1: Data Merging Basics\n","\n","Learn how you can merge disparate data using inner joins. By combining information from multiple sources you’ll uncover compelling insights that may have previously been hidden. You’ll also learn how the relationship between those sources, such as one-to-one or one-to-many, can affect your result."],"metadata":{"id":"ta0qQivc01p9"}},{"cell_type":"markdown","source":["# Inner join\n","# What column to merge on?\n","Chicago provides a list of taxicab owners and vehicles licensed to operate within the city, for public safety. Your goal is to merge two tables together. One table is called taxi_owners, with info about the taxi cab company owners, and one is called taxi_veh, with info about each taxi cab vehicle. Both the taxi_owners and taxi_veh tables have been loaded for you and you can explore them in the IPython shell.\n","\n","Choose the column you would use to merge the two tables on using the .merge() method.\n","\n","## Instructions\n","\n","Possible answers\n","\n","\n","on='rid'\n","\n","**on='vid'**\n","\n","on='year'\n","\n","on='zip'"],"metadata":{"id":"pINX63EG1A3m"}},{"cell_type":"markdown","source":["Yes, great job! Both DataFrames contained the column vid. Now continue on to the next exercise where you will using this information to merge the tables."],"metadata":{"id":"wkL3NMOL1JoW"}},{"cell_type":"markdown","source":["# Your first inner join\n","You have been tasked with figuring out what the most popular types of fuel used in Chicago taxis are. To complete the analysis, you need to merge the taxi_owners and taxi_veh tables together on the vid column. You can then use the merged table along with the .value_counts() method to find the most common fuel_type.\n","\n","Since you'll be working with pandas throughout the course, the package will be preloaded for you as pd in each exercise in this course. Also the taxi_owners and taxi_veh DataFrames are loaded for you.\n","\n","## Instructions 1/3\n","\n","Merge taxi_owners with taxi_veh on the column vid, and save the result to taxi_own_veh.\n","## 2/3\n","Set the left and right table suffixes for overlapping columns of the merge to _own and _veh, respectively.\n","## 3/3\n","Select the fuel_type column from taxi_own_veh and print the value_counts() to find the most popular fuel_types used."],"metadata":{"id":"6yKrwcBU1f11"}},{"cell_type":"code","source":["# Merge the taxi_owners and taxi_veh tables\n","taxi_own_veh = taxi_owners.merge(taxi_veh, on = \"vid\")\n","\n","# Print the column names of the taxi_own_veh\n","print(taxi_own_veh.columns)\n"],"metadata":{"id":"bDyOOrZY1lNY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Index(['rid', 'vid', 'owner_x', 'address', 'zip', 'make', 'model', 'year', 'fuel_type', 'owner_y'], dtype='object')"],"metadata":{"id":"FUwczFEV1oxN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge the taxi_owners and taxi_veh tables setting a suffix\n","taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes = (\"_own\", \"_veh\"))\n","\n","# Print the column names of taxi_own_veh\n","print(taxi_own_veh.columns)"],"metadata":{"id":"mnavqcsj1-3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Index(['rid', 'vid', 'owner_own', 'address', 'zip', 'make', 'model', 'year', 'fuel_type', 'owner_veh'], dtype='object')"],"metadata":{"id":"0ddKf15H2DeS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge the taxi_owners and taxi_veh tables setting a suffix\n","taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes=('_own','_veh'))\n","\n","# Print the value_counts to find the most popular fuel_type\n","print(taxi_own_veh['fuel_type'].value_counts())"],"metadata":{"id":"Hl9OMyeP2FBa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    Index(['rid', 'vid', 'owner_own', 'address', 'zip', 'make', 'model', 'year', 'fuel_type', 'owner_veh'], dtype='object')\n","\n","<script.py> output:\n","    HYBRID                    2792\n","    GASOLINE                   611\n","    FLEX FUEL                   89\n","    COMPRESSED NATURAL GAS      27\n","    Name: fuel_type, dtype: int64"],"metadata":{"id":"MnNz_-Yv2Klo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bravo! You correctly merged the two tables together and found out that the most common fuel type for taxis in Chicago are hybrids."],"metadata":{"id":"tcaRmuDV2Ne7"}},{"cell_type":"markdown","source":["# Inner joins and number of rows returned\n","All of the merges you have studied to this point are called inner joins. It is necessary to understand that inner joins only return the rows with matching values in both tables. You will explore this further by reviewing the merge between the wards and census tables, then comparing it to merges of copies of these tables that are slightly altered, named wards_altered, and census_altered. The first row of the wards column has been changed in the altered tables. You will examine how this affects the merge between them. The tables have been loaded for you.\n","\n","For this exercise, it is important to know that the wards and census tables start with 50 rows.\n","\n","## Instructions 1/3\n","\n","Merge wards and census on the ward column and save the result to wards_census.\n","\n","## 2/3\n","Merge the wards_altered and census tables on the ward column, and notice the difference in returned rows.\n","## 3/3\n","Merge the wards and census_altered tables on the ward column, and notice the difference in returned rows."],"metadata":{"id":"-_h09EUC2USy"}},{"cell_type":"code","source":["# Merge the wards and census tables on the ward column\n","wards_census = wards.merge(census, on = \"ward\")\n","\n","# Print the shape of wards_census\n","print('wards_census table shape:', wards_census.shape)"],"metadata":{"id":"PJTNzR3I2fUT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    wards_census table shape: (50, 9)"],"metadata":{"id":"FJxMuhBd2jN2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the first few rows of the wards_altered table to view the change\n","print(wards_altered[['ward']].head())\n","\n","# Merge the wards_altered and census tables on the ward column\n","wards_altered_census = wards_altered.merge(census, on = \"ward\")\n","\n","# Print the shape of wards_altered_census\n","print('wards_altered_census table shape:', wards_altered_census.shape)\n"],"metadata":{"id":"WFlSgLUr2kOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","      ward\n","    0   61\n","    1    2\n","    2    3\n","    3    4\n","    4    5\n","    wards_altered_census table shape: (49, 9)"],"metadata":{"id":"XIvOW_Eb2oHg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the first few rows of the census_altered table to view the change\n","print(census_altered[['ward']].head())\n","\n","# Merge the wards and census_altered tables on the ward column\n","wards_census_altered = wards.merge(census_altered, on = \"ward\")\n","\n","# Print the shape of wards_census_altered\n","print('wards_census_altered table shape:', wards_census_altered.shape)"],"metadata":{"id":"gt_PKIUX2q9C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","      ward\n","    0   61\n","    1    2\n","    2    3\n","    3    4\n","    4    5\n","    wards_altered_census table shape: (49, 9)\n","\n","<script.py> output:\n","       ward\n","    0  None\n","    1     2\n","    2     3\n","    3     4\n","    4     5\n","    wards_census_altered table shape: (49, 9)"],"metadata":{"id":"q_GZZdGQ2t5y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great job! In step 1, the .merge() returned a table with the same number of rows as the original wards table. However, in steps 2 and 3, using the altered tables with the altered first row of the ward column, the number of returned rows was fewer. There was not a matching value in the ward column of the other table. _Remember that .merge() only returns rows where the values match in both tables._"],"metadata":{"id":"EvUvf2dt2wcZ"}},{"cell_type":"markdown","source":["# One-to-many relationships\n","# One-to-many merge\n","A business may have one or multiple owners. In this exercise, you will continue to gain experience with one-to-many merges by merging a table of business owners, called biz_owners, to the licenses table. Recall from the video lesson, with a one-to-many relationship, a row in the left table may be repeated if it is related to multiple rows in the right table. In this lesson, you will explore this further by finding out what is the most common business owner title. (i.e., secretary, CEO, or vice president)\n","\n","The licenses and biz_owners DataFrames are loaded for you.\n","\n","## Instructions\n","\n","Starting with the licenses table on the left, merge it to the biz_owners table on the column account, and save the results to a variable named licenses_owners.\n","Group licenses_owners by title and count the number of accounts for each title. Save the result as counted_df\n","Sort counted_df by the number of accounts in descending order, and save this as a variable named sorted_df.\n","Use the .head() method to print the first few rows of the sorted_df."],"metadata":{"id":"AMfWQuOG2zR0"}},{"cell_type":"code","source":["# Merge the licenses and biz_owners table on account\n","licenses_owners = licenses.merge(biz_owners, on = \"account\")\n","\n","# Group the results by title then count the number of accounts\n","counted_df = licenses_owners.groupby(\"title\").agg({'account':'count'})\n","\n","# Sort the counted_df in desending order\n","sorted_df = counted_df.sort_values(by = [\"account\"], ascending=False )\n","\n","# Use .head() method to print the first few rows of sorted_df\n","print(sorted_df.head())"],"metadata":{"id":"5RjqsZaE2_aI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","                     account\n","    title\n","    PRESIDENT           6259\n","    SECRETARY           5205\n","    SOLE PROPRIETOR     1658\n","    OTHER               1200\n","    VICE PRESIDENT       970"],"metadata":{"id":"_QuSf8bu3CAD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wonderful! After merging the tables together, you counted the number of repeated rows with the combination of .groupby() and .agg() statements. You see that president, followed by secretary, are the most common business owner titles."],"metadata":{"id":"Cjt_NdVB3C8g"}},{"cell_type":"markdown","source":["# Merging multiple DataFrames\n","\n","# Total riders in a month\n","Your goal is to find the total number of rides provided to passengers passing through the Wilson station (station_name == 'Wilson') when riding Chicago's public transportation system on weekdays (day_type == 'Weekday') in July (month == 7). Luckily, Chicago provides this detailed data, but it is in three different tables. You will work on merging these tables together to answer the question. This data is different from the business related data you have seen so far, but all the information you need to answer the question is provided.\n","\n","The cal, ridership, and stations DataFrames have been loaded for you. The relationship between the tables can be seen in the diagram below.\n","\n","Table diagram. The cal table relates to ridership via year, month, and day. The ridership table relates to the stations table via station_id.\n","\n","## Instructions 1/3\n","\n","Merge the ridership and cal tables together, starting with the ridership table on the left and save the result to the variable ridership_cal. If you code takes too long to run, your merge conditions might be incorrect.\n","\n"],"metadata":{"id":"Xvr_gqaH3HIV"}},{"cell_type":"code","source":["# Merge the ridership and cal tables\n","ridership_cal = ridership.merge(cal, on = [\"year\", \"month\", \"day\"])"],"metadata":{"id":"b6Ie0Yid3RAj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## 2/3\n","Extend the previous merge to three tables by also merging the stations table."],"metadata":{"id":"QrGigYhi3Wp4"}},{"cell_type":"code","source":["# Merge the ridership, cal, and stations tables\n","ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \\\n","            \t\t\t\t.merge(stations, on = \"station_id\")\n","print(ridership_cal_stations.head())"],"metadata":{"id":"GaXl9QIn3a8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","      station_id  year  month  day  rides        day_type        station_name                 location\n","    0      40010  2019      1    1    576  Sunday/Holiday  Austin-Forest Park  (41.870851, -87.776812)\n","    1      40010  2019      1    2   1457         Weekday  Austin-Forest Park  (41.870851, -87.776812)\n","    2      40010  2019      1    3   1543         Weekday  Austin-Forest Park  (41.870851, -87.776812)\n","    3      40010  2019      1    4   1621         Weekday  Austin-Forest Park  (41.870851, -87.776812)\n","    4      40010  2019      1    5    719        Saturday  Austin-Forest Park  (41.870851, -87.776812)"],"metadata":{"id":"kDI6j4x13huA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3/3\n","\n","Create a variable called filter_criteria to select the appropriate rows from the merged table so that you can sum the rides column"],"metadata":{"id":"f6HMeHD53mni"}},{"cell_type":"code","source":["# Merge the ridership, cal, and stations tables\n","ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \\\n","\t\t\t\t\t\t\t.merge(stations, on='station_id')\n","\n","# Create a filter to filter ridership_cal_stations\n","filter_criteria = ((ridership_cal_stations['month'] == 7)\n","                   & (ridership_cal_stations['day_type'] == \"Weekday\")\n","                   & (ridership_cal_stations['station_name'] == \"Wilson\"))\n","\n","# Use .loc and the filter to select for rides\n","print(ridership_cal_stations.loc[filter_criteria, 'rides'].sum())"],"metadata":{"id":"rrKzEo883uFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","      station_id  year  month  day  rides        day_type        station_name                 location\n","    0      40010  2019      1    1    576  Sunday/Holiday  Austin-Forest Park  (41.870851, -87.776812)\n","    1      40010  2019      1    2   1457         Weekday  Austin-Forest Park  (41.870851, -87.776812)\n","    2      40010  2019      1    3   1543         Weekday  Austin-Forest Park  (41.870851, -87.776812)\n","    3      40010  2019      1    4   1621         Weekday  Austin-Forest Park  (41.870851, -87.776812)\n","    4      40010  2019      1    5    719        Saturday  Austin-Forest Park  (41.870851, -87.776812)\n","\n","<script.py> output:\n","    140005"],"metadata":{"id":"4G8MtLsM3ytA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Awesome work! You merged three DataFrames together, including merging two tables on multiple columns. Once the tables were merged, you filtered and selected just like any other DataFrame. Finally, you found out that the Wilson station had 140,005 riders during weekdays in July."],"metadata":{"id":"yUACvdGC30Bq"}},{"cell_type":"markdown","source":["# Three table merge\n","To solidify the concept of a three DataFrame merge, practice another exercise. A reasonable extension of our review of Chicago business data would include looking at demographics information about the neighborhoods where the businesses are. A table with the median income by zip code has been provided to you. You will merge the licenses and wards tables with this new income-by-zip-code table called zip_demo.\n","\n","The licenses, wards, and zip_demo DataFrames have been loaded for you.\n","\n","## Instructions\n","\n","Starting with the licenses table, merge to it the zip_demo table on the zip column. Then merge the resulting table to the wards table on the ward column. Save result of the three merged tables to a variable named licenses_zip_ward.\n","Group the results of the three merged tables by the column alderman and find the median income."],"metadata":{"id":"UBDI6I_d35nH"}},{"cell_type":"code","source":["# Merge licenses and zip_demo, on zip; and merge the wards on ward\n","licenses_zip_ward = licenses.merge(zip_demo, on=\"zip\") \\\n","\t\t\t\t\t\t\t.merge(wards, on=\"ward\")\n","# Print the results by alderman and show median income\n","print(licenses_zip_ward.groupby(\"alderman\").agg({'income':'median'}))\n","print(licenses_zip_ward.head())"],"metadata":{"id":"LHbB-D2u39KT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","                                 income\n","    alderman\n","    Ameya Pawar                 66246.0\n","    Anthony A. Beale            38206.0\n","    Anthony V. Napolitano       82226.0\n","    Ariel E. Reyboras           41307.0\n","    Brendan Reilly             110215.0\n","    Brian Hopkins               87143.0\n","    Carlos Ramirez-Rosa         66246.0\n","    Carrie M. Austin            38206.0\n","    Chris Taliaferro            55566.0\n","    Daniel \"Danny\" Solis        41226.0\n","    David H. Moore              33304.0\n","    Deborah Mell                66246.0\n","    Debra L. Silverstein        50554.0\n","    Derrick G. Curtis           65770.0\n","    Edward M. Burke             42335.0\n","    Emma M. Mitts               36283.0\n","    George Cardenas             33959.0\n","    Gilbert Villegas            41307.0\n","    Gregory I. Mitchell         24941.0\n","    Harry Osterman              45442.0\n","    Howard B. Brookins, Jr.     33304.0\n","    James Cappleman             79565.0\n","    Jason C. Ervin              41226.0\n","    Joe Moore                   39163.0\n","    John S. Arena               70122.0\n","    Leslie A. Hairston          28024.0\n","    Margaret Laurino            70122.0\n","    Marty Quinn                 67045.0\n","    Matthew J. O'Shea           59488.0\n","    Michael R. Zalewski         42335.0\n","    Michael Scott, Jr.          31445.0\n","    Michelle A. Harris          32558.0\n","    Michelle Smith             100116.0\n","    Milagros \"Milly\" Santiago   41307.0\n","    Nicholas Sposato            62223.0\n","    Pat Dowell                  46340.0\n","    Patrick Daley Thompson      41226.0\n","    Patrick J. O'Connor         50554.0\n","    Proco \"Joe\" Moreno          87143.0\n","    Raymond A. Lopez            33959.0\n","    Ricardo Munoz               31445.0\n","    Roberto Maldonado           68223.0\n","    Roderick T. Sawyer          32558.0\n","    Scott Waguespack            68223.0\n","    Susan Sadlowski Garza       38417.0\n","    Tom Tunney                  88708.0\n","    Toni L. Foulkes             27573.0\n","    Walter Burnett, Jr.         87143.0\n","    William D. Burns           107811.0\n","    Willie B. Cochran           28024.0\n","      account ward  aid               business                address_x  zip_x  income    alderman                address_y  zip_y\n","    0  307071    3  743   REGGIE'S BAR & GRILL          2105 S STATE ST  60616   46340  Pat Dowell  5046 SOUTH STATE STREET  60609\n","    1   11280    3  763              PRIME WAY      2251 S STATE ST 1ST  60616   46340  Pat Dowell  5046 SOUTH STATE STREET  60609\n","    2   15015    3  NaN  SOUTHVIEW MANOR, INC.      3311 S MICHIGAN AVE  60616   46340  Pat Dowell  5046 SOUTH STATE STREET  60609\n","    3   19168    3  666               BP AMOCO  3101 S MICHIGAN AVE 1ST  60616   46340  Pat Dowell  5046 SOUTH STATE STREET  60609\n","    4  205980    3  763  J & J  FISH & CHICKEN            8 E CERMAK RD  60616   46340  Pat Dowell  5046 SOUTH STATE STREET  60609"],"metadata":{"id":"e7utkxZR4CgJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nice work! You successfully merged three tables together. With the merged data, you can complete your income analysis. You see that only a few aldermen represent businesses in areas where the median income is greater than $62,000, which is the median income for the state of Illinois."],"metadata":{"id":"CAlqEYjs4G43"}},{"cell_type":"markdown","source":["# One-to-many merge with multiple tables\n","In this exercise, assume that you are looking to start a business in the city of Chicago. Your perfect idea is to start a company that uses goats to mow the lawn for other businesses. However, you have to choose a location in the city to put your goat farm. You need a location with a great deal of space and relatively few businesses and people around to avoid complaints about the smell. You will need to merge three tables to help you choose your location. The land_use table has info on the percentage of vacant land by city ward. The census table has population by ward, and the licenses table lists businesses by ward.\n","\n","The land_use, census, and licenses tables have been loaded for you.\n","\n","## Instructions 1/3\n","\n","Merge land_use and census on the ward column. Merge the result of this with licenses on the ward column, using the suffix _cen for the left table and _lic for the right table. Save this to the variable land_cen_lic.\n"],"metadata":{"id":"lOh-Vq8d4MLK"}},{"cell_type":"code","source":["# Merge land_use and census and merge result with licenses including suffixes\n","land_cen_lic = land_use.merge(census, on = \"ward\")\\\n","                            .merge(licenses, on = \"ward\", suffixes = [\"_cen\", \"_lic\"])"],"metadata":{"id":"zVfqjv374RaE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2/3\n","Group land_cen_lic by ward, pop_2010 (the population in 2010), and vacant, then count the number of accounts. Save the results to pop_vac_lic.\n","\n","Hint\n","\n","To group by multiple DataFrame columns, enter them as a list to the .groupby() method."],"metadata":{"id":"zhxQ1RQN4Y1E"}},{"cell_type":"code","source":["# Merge land_use and census and merge result with licenses including suffixes\n","land_cen_lic = land_use.merge(census, on='ward') \\\n","                    .merge(licenses, on='ward', suffixes=('_cen','_lic'))\n","\n","# Group by ward, pop_2010, and vacant, then count the # of accounts\n","pop_vac_lic = land_cen_lic.groupby([\"ward\", \"pop_2010\", \"vacant\"], as_index=False).agg({'account':'count'})\n","print(pop_vac_lic)"],"metadata":{"id":"boQXXANK4ixj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3/3\n","\n","Sort pop_vac_lic by vacant, account, andpop_2010 in descending, ascending, and ascending order respectively. Save it as sorted_pop_vac_lic."],"metadata":{"id":"mGT_DS-44rFt"}},{"cell_type":"code","source":["# Merge land_use and census and merge result with licenses including suffixes\n","land_cen_lic = land_use.merge(census, on='ward') \\\n","                    .merge(licenses, on='ward', suffixes=('_cen','_lic'))\n","\n","# Group by ward, pop_2010, and vacant, then count the # of accounts\n","pop_vac_lic = land_cen_lic.groupby(['ward','pop_2010','vacant'],\n","                                   as_index=False).agg({'account':'count'})\n","\n","# Sort pop_vac_lic and print the results\n","sorted_pop_vac_lic = pop_vac_lic.sort_values([\"vacant\", \"account\", \"pop_2010\"],\n","                                             ascending=[False, True, True])\n","\n","# Print the top few rows of sorted_pop_vac_lic\n","print(sorted_pop_vac_lic.head())"],"metadata":{"id":"9CUWuECm4yUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","       ward  pop_2010  vacant  account\n","    0     1     56149       2      253\n","    1    10     51535      14      130\n","    2    11     51497       5      201\n","    3    12     52235       4      255\n","    4    13     53722       1      101\n","    5    14     54031       3      202\n","    6    15     51501       6      103\n","    7    16     51954      13      156\n","    8    17     51846       5      109\n","    9    18     52992       3      124\n","    10   19     51525       0      125\n","    11    2     55805       2      301\n","    12   20     52372      15      123\n","    13   21     51632       5      123\n","    14   22     53515       7      156\n","    15   23     53728       1      147\n","    16   24     54909      13       98\n","    17   25     54539       5      321\n","    18   26     53516       2      175\n","    19   27     52939       7      497\n","    20   28     55199      11      189\n","    21   29     55267       2      154\n","    22    3     53039      13      173\n","    23   30     55560       1      186\n","    24   31     53724       0      173\n","    25   32     55184       1      248\n","    26   33     55598       1      208\n","    27   34     51599       7       99\n","    28   35     55281       1      169\n","    29   36     54766       1      145\n","    30   37     51538       6      173\n","    31   38     56001       0      136\n","    32   39     55882       1      219\n","    33    4     54589       7      160\n","    34   40     55319       0      194\n","    35   41     55991       0      153\n","    36   42     55870       1     1371\n","    37   43     56170       1      232\n","    38   44     56058       0      238\n","    39   45     55967       0      217\n","    40   46     53784       1      143\n","    41   47     55074       0      275\n","    42   48     55014       1      156\n","    43   49     54633       0      111\n","    44    5     51455       3      104\n","    45   50     55809       1      168\n","    46    6     52341       8      149\n","    47    7     51581      19       80\n","    48    8     51687       5      176\n","    49    9     51519       6      101\n","\n","<script.py> output:\n","       ward  pop_2010  vacant  account\n","    47    7     51581      19       80\n","    12   20     52372      15      123\n","    1    10     51535      14      130\n","    16   24     54909      13       98\n","    7    16     51954      13      156"],"metadata":{"id":"ByBQnR8141c7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great job putting your new skills into action. You merged multiple tables with varying relationships and added suffixes to make your column names clearer. Using your skills, you were able to pull together information from different tables to see that the 7th ward would be a good place to build your goat farm!"],"metadata":{"id":"bQ-tJ47k5ARJ"}},{"cell_type":"markdown","source":["# Chapter 2: Merging Tables With Different Join Types\n","\n","Take your knowledge of joins to the next level. In this chapter, you’ll work with TMDb movie data as you learn about left, right, and outer joins. You’ll also discover how to merge a table to itself and merge on a DataFrame index."],"metadata":{"id":"2eEDatW45FXs"}},{"cell_type":"markdown","source":["# Left join\n","# Counting missing rows with left join\n","The Movie Database is supported by volunteers going out into the world, collecting data, and entering it into the database. This includes financial data, such as movie budget and revenue. If you wanted to know which movies are still missing data, you could use a left join to identify them. Practice using a left join by merging the movies table and the financials table.\n","\n","The movies and financials tables have been loaded for you.\n","\n","## Instructions 1/3\n","\n","Question\n","What column is likely the best column to merge the two tables on?\n","\n","Possible answers\n","\n","\n","on='budget'\n","\n","on='popularity'\n","\n","**on='id'**"],"metadata":{"id":"cAE4mcHR5YCM"}},{"cell_type":"markdown","source":["# 2/3\n","Merge the movies table, as the left table, with the financials table using a left join, and save the result to movies_financials.\n","\n","Hint\n","\n","The .merge() method should have a pattern similar to dfl.merge(df2, on='col', how='join_type').\n","Setting the how argument of the .merge() method to the key word 'left' will cause it to perform a left join."],"metadata":{"id":"EsiT_vWR5xPL"}},{"cell_type":"code","source":["# Merge movies and financials with a left join\n","movies_financials = movies.merge(financials, on = \"id\", how = \"left\")\n"],"metadata":{"id":"GCtKQdQc6FtC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3/3\n","Count the number of rows in movies_financials with a null value in the budget column.\n","\n","Hint\n","\n","You can use the .isnull() method to return a Boolean index if a column is null, and the .sum() method to count a Boolean index for the number of True values."],"metadata":{"id":"7s9sw2zz6KDL"}},{"cell_type":"code","source":["# Merge the movies table with the financials table with a left join\n","movies_financials = movies.merge(financials, on='id', how='left')\n","\n","# Count the number of rows in the budget column that are missing\n","number_of_missing_fin = movies_financials['budget'].isnull().sum()\n","\n","# Print the number of movies missing financials\n","print(number_of_missing_fin)"],"metadata":{"id":"idBHa8Iq6Oxm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great job! You used a left join to find out which rows in the financials table were missing data. When performing a left join, the .merge() method returns a row full of null values for columns in the right table if the key column does not have a matching value in both tables. We see that there are at least 1,500 rows missing data. Wow! That sounds like a lot of work."],"metadata":{"id":"z7A-0pma6ShB"}},{"cell_type":"markdown","source":["# Enriching a dataset\n","Setting how='left' with the .merge()method is a useful technique for enriching or enhancing a dataset with additional information from a different table. In this exercise, you will start off with a sample of movie data from the movie series Toy Story. Your goal is to enrich this data by adding the marketing tag line for each movie. You will compare the results of a left join versus an inner join.\n","\n","The toy_story DataFrame contains the Toy Story movies. The toy_story and taglines DataFrames have been loaded for you.\n","\n","## Instructions 1/2\n","\n","Merge toy_story and taglines on the id column with a left join, and save the result as toystory_tag.\n","\n","## 2/3\n","\n","With toy_story as the left table, merge to it taglines on the id column with an inner join, and save as toystory_tag."],"metadata":{"id":"FW_maLf26XaF"}},{"cell_type":"code","source":["# Merge the toy_story and taglines tables with a left join\n","toystory_tag = toy_story.merge(taglines, on=\"id\", how=\"left\")\n","\n","# Print the rows and shape of toystory_tag\n","print(toystory_tag)\n","print(toystory_tag.shape)"],"metadata":{"id":"K7HCwO4D6jRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","          id        title  popularity release_date                   tagline\n","    0  10193  Toy Story 3      59.995   2010-06-16  No toy gets left behind.\n","    1    863  Toy Story 2      73.575   1999-10-30        The toys are back!\n","    2    862    Toy Story      73.640   1995-10-30                       NaN\n","    (3, 5)"],"metadata":{"id":"GJzwApFX6mwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge the toy_story and taglines tables with a inner join\n","toystory_tag = toy_story.merge(taglines, on = \"id\")\n","\n","# Print the rows and shape of toystory_tag\n","print(toystory_tag)\n","print(toystory_tag.shape)"],"metadata":{"id":"0NFcSILN6nwK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","          id        title  popularity release_date                   tagline\n","    0  10193  Toy Story 3      59.995   2010-06-16  No toy gets left behind.\n","    1    863  Toy Story 2      73.575   1999-10-30        The toys are back!\n","    2    862    Toy Story      73.640   1995-10-30                       NaN\n","    (3, 5)\n","\n","<script.py> output:\n","          id        title  popularity release_date                   tagline\n","    0  10193  Toy Story 3      59.995   2010-06-16  No toy gets left behind.\n","    1    863  Toy Story 2      73.575   1999-10-30        The toys are back!\n","    (2, 5)"],"metadata":{"id":"r-HZFFFF6rhc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["That's fantastic work! If your goal is to enhance or enrich a dataset, then you do not want to lose any of your original data. A left join will do that by returning all of the rows of your left table, while using an inner join may result in lost data if it does not exist in both tables."],"metadata":{"id":"6LTC_rgb60K1"}},{"cell_type":"markdown","source":["# How many rows with a left join?\n","Select the true statement about left joins.\n","\n","Try running the following code statements in the IPython shell.\n","\n","left_table.merge(one_to_one, on='id', how='left').shape\n","left_table.merge(one_to_many, on='id', how='left').shape\n","Note that the left_table starts out with 4 rows.\n","\n","## Instructions\n","\n","Possible answers\n","\n","\n","The output of a one-to-one merge with a left join will have more rows than the left table.\n","\n","The output of a one-to-one merge with a left join will have fewer rows than the left table.\n","\n","**The output of a one-to-many merge with a left join will have greater than or equal rows than the left table.**"],"metadata":{"id":"I3hNnz4064ur"}},{"cell_type":"markdown","source":["That's correct! A left join will return all of the rows from the left table. If those rows in the left table match multiple rows in the right table, then all of those rows will be returned. Therefore, the returned rows must be equal to if not greater than the left table. Knowing what to expect is useful in troubleshooting any suspicious merges."],"metadata":{"id":"9kvYgVTb7Bjl"}},{"cell_type":"markdown","source":["# Other joins\n","\n","Right join to find unique **movies**\n","Most of the recent big-budget science fiction movies can also be classified as action movies. You are given a table of science fiction movies called scifi_movies and another table of action movies called action_movies. Your goal is to find which movies are considered only science fiction movies. Once you have this table, you can merge the movies table in to see the movie names. Since this exercise is related to science fiction movies, use a right join as your superhero power to solve this problem.\n","\n","The movies, scifi_movies, and action_movies tables have been loaded for you.\n","\n","## Instructions 1/4\n","\n","Merge action_movies and scifi_movies tables with a right join on movie_id. Save the result as action_scifi.\n","## 2/4\n","Update the merge to add suffixes, where '_act' and '_sci' are suffixes for the left and right tables, respectively.\n","## 3/4\n","From action_scifi, subset only the rows where the genre_act column is null.\n","\n","# 4/4\n","Merge movies and scifi_only using the id column in the left table and the movie_id column in the right table with an inner join."],"metadata":{"id":"ZqLgOETI7IRq"}},{"cell_type":"code","source":["# Merge action_movies to scifi_movies with right join\n","action_scifi = action_movies.merge(scifi_movies, on = \"movie_id\", how = \"right\")"],"metadata":{"id":"S0q1SYK_7hn7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge action_movies to scifi_movies with right join\n","action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n","                                   suffixes = [\"_act\", \"_sci\"])\n","\n","# Print the first few rows of action_scifi to see the structure\n","print(action_scifi.head())"],"metadata":{"id":"b83rzO917kiF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","       movie_id genre_act        genre_sci\n","    0        11    Action  Science Fiction\n","    1        18    Action  Science Fiction\n","    2        19       NaN  Science Fiction\n","    3        38       NaN  Science Fiction\n","    4        62       NaN  Science Fiction"],"metadata":{"id":"D4gWUNP87ojy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge action_movies to the scifi_movies with right join\n","action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n","                                   suffixes=('_act','_sci'))\n","\n","# From action_scifi, select only the rows where the genre_act column is null\n","scifi_only = action_scifi[action_scifi[\"genre_act\"].isnull() == True]\n","print(scifi_only)"],"metadata":{"id":"f8xHOP4Z7rQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","         movie_id genre_act        genre_sci\n","    2          19       NaN  Science Fiction\n","    3          38       NaN  Science Fiction\n","    4          62       NaN  Science Fiction\n","    5          68       NaN  Science Fiction\n","    6          74       NaN  Science Fiction\n","    ..        ...       ...              ...\n","    529    333371       NaN  Science Fiction\n","    530    335866       NaN  Science Fiction\n","    531    347548       NaN  Science Fiction\n","    532    360188       NaN  Science Fiction\n","    534    371690       NaN  Science Fiction\n","\n","    [258 rows x 3 columns]"],"metadata":{"id":"njNVXD_m7vmH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge action_movies to the scifi_movies with right join\n","action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n","                                   suffixes=('_act','_sci'))\n","\n","# From action_scifi, select only the rows where the genre_act column is null\n","scifi_only = action_scifi[action_scifi['genre_act'].isnull()]\n","\n","# Merge the movies and scifi_only tables with an inner join\n","movies_and_scifi_only = movies.merge(scifi_only, left_on = \"id\", right_on = \"movie_id\")\n","\n","# Print the first few rows and shape of movies_and_scifi_only\n","print(movies_and_scifi_only.head())\n","print(movies_and_scifi_only.shape)"],"metadata":{"id":"n9Ou9eYg71Hy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","         movie_id genre_act        genre_sci\n","    2          19       NaN  Science Fiction\n","    3          38       NaN  Science Fiction\n","    4          62       NaN  Science Fiction\n","    5          68       NaN  Science Fiction\n","    6          74       NaN  Science Fiction\n","    ..        ...       ...              ...\n","    529    333371       NaN  Science Fiction\n","    530    335866       NaN  Science Fiction\n","    531    347548       NaN  Science Fiction\n","    532    360188       NaN  Science Fiction\n","    534    371690       NaN  Science Fiction\n","\n","    [258 rows x 3 columns]\n","\n","<script.py> output:\n","          id                         title  popularity release_date  movie_id genre_act        genre_sci\n","    0  18841  The Lost Skeleton of Cadavra       1.681   2001-09-12     18841       NaN  Science Fiction\n","    1  26672     The Thief and the Cobbler       2.439   1993-09-23     26672       NaN  Science Fiction\n","    2  15301      Twilight Zone: The Movie      12.903   1983-06-24     15301       NaN  Science Fiction\n","    3   8452                   The 6th Day      18.447   2000-11-17      8452       NaN  Science Fiction\n","    4   1649    Bill & Ted's Bogus Journey      11.350   1991-07-19      1649       NaN  Science Fiction\n","    (258, 7)"],"metadata":{"id":"AEo77B1m749r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Well done, right join to the rescue! You found over 250 action only movies by merging action_movies and scifi_movies using a right join. With this, you were able to find the rows not found in the action_movies table. Additionally, you used the left_on and right_on arguments to merge in the movies table. Wow! You are a superhero."],"metadata":{"id":"c7g6Q3yP77QD"}},{"cell_type":"markdown","source":["# Popular genres with right join\n","What are the genres of the most popular movies? To answer this question, you need to merge data from the movies and movie_to_genres tables. In a table called pop_movies, the top 10 most popular movies in the movies table have been selected. To ensure that you are analyzing all of the popular movies, merge it with the movie_to_genres table using a right join. To complete your analysis, count the number of different genres. Also, the two tables can be merged by the movie ID. However, in pop_movies that column is called id, and in movies_to_genres it's called movie_id.\n","\n","The pop_movies and movie_to_genres tables have been loaded for you.\n","\n","## Instructions\n","\n","Merge movie_to_genres and pop_movies using a right join. Save the results as genres_movies.\n","Group genres_movies by genre and count the number of id values."],"metadata":{"id":"lpSIV-n58Auc"}},{"cell_type":"code","source":["# Use right join to merge the movie_to_genres and pop_movies tables\n","genres_movies = movie_to_genres.merge(pop_movies, how='right',\n","                                      left_on = \"movie_id\",\n","                                      right_on = \"id\")\n","\n","# Count the number of genres\n","genre_count = genres_movies.groupby('genre').agg({'id':'count'})\n","\n","# Plot a bar chart of the genre_count\n","genre_count.plot(kind='bar')\n","plt.show()"],"metadata":{"id":"ZVT7q42Y8GDD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nice job! The right join ensured that you were analyzing all of the pop_movies. You see from the results that adventure and action are the most popular genres."],"metadata":{"id":"X8Zg1VQF8ZJc"}},{"cell_type":"markdown","source":["# Using outer join to select actors\n","One cool aspect of using an outer join is that, because it returns all rows from both merged tables and null where they do not match, you can use it to find rows that do not have a match in the other table. To try for yourself, you have been given two tables with a list of actors from two popular movies: Iron Man 1 and Iron Man 2. Most of the actors played in both movies. Use an outer join to find actors who did not act in both movies.\n","\n","The Iron Man 1 table is called iron_1_actors, and Iron Man 2 table is called iron_2_actors. Both tables have been loaded for you and a few rows printed so you can see the structure.\n","\n","Venn graph with no overlap\n","\n","## Instructions\n","1\n","Save to iron_1_and_2 the merge of iron_1_actors (left) with iron_2_actors tables with an outer join on the id column, and set suffixes to ('_1','_2').\n","Create an index that returns True if name_1 or name_2 are null, and False otherwise."],"metadata":{"id":"QQ3S2VHa8f5O"}},{"cell_type":"code","source":["# Merge iron_1_actors to iron_2_actors on id with outer join using suffixes\n","iron_1_and_2 = iron_1_actors.merge(iron_2_actors,\n","                                     on = \"id\",\n","                                     how = \"outer\",\n","                                     suffixes= [\"_1\", \"_2\"])\n","\n","# Create an index that returns true if name_1 or name_2 are null\n","m = ((iron_1_and_2['name_1'].isnull()) |\n","     (iron_1_and_2['name_2'].isnull()))\n","\n","# Print the first few rows of iron_1_and_2\n","print(iron_1_and_2[m].head())"],"metadata":{"id":"OuriFqod8lKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["iron_1_actors #########\n","                 character     id             name\n","3                   Yinsen  17857       Shaun Toub\n","4  Virginia \"Pepper\" Potts  12052  Gwyneth Paltrow\n","\n","iron_2_actors #########\n","                                          character    id                name\n","4                             Ivan Vanko / Whiplash  2295       Mickey Rourke\n","3  Natalie Rushman / Natasha Romanoff / Black Widow  1245  Scarlett Johansson\n","\n","<script.py> output:\n","                       character_1      id           name_1 character_2 name_2\n","    0                       Yinsen   17857       Shaun Toub         NaN    NaN\n","    2  Obadiah Stane / Iron Monger    1229     Jeff Bridges         NaN    NaN\n","    3                  War Machine   18288  Terrence Howard         NaN    NaN\n","    5                         Raza   57452      Faran Tahir         NaN    NaN\n","    8                   Abu Bakaar  173810    Sayed Badreya         NaN    NaN"],"metadata":{"id":"4O6WRruH8pvz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nice job! Using an outer join, you were able to pick only those rows where the actor played in only one of the two movies."],"metadata":{"id":"DnQpZ4Rq8wMN"}},{"cell_type":"markdown","source":["# Merging a table to itself\n","# Self join\n","Merging a table to itself can be useful when you want to compare values in a column to other values in the same column. In this exercise, you will practice this by creating a table that for each movie will list the movie director and a member of the crew on one row. You have been given a table called crews, which has columns id, job, and name. First, merge the table to itself using the movie ID. This merge will give you a larger table where for each movie, every job is matched against each other. Then select only those rows with a director in the left table, and avoid having a row where the director's job is listed in both the left and right tables. This filtering will remove job combinations that aren't with the director.\n","\n","The crews table has been loaded for you.\n","\n","## Instructions 1/3\n","\n","To a variable called crews_self_merged, merge the crews table to itself on the id column using an inner join, setting the suffixes to '_dir' and '_crew' for the left and right tables respectively."],"metadata":{"id":"7QBM48Jc-nAl"}},{"cell_type":"code","source":["# Merge the crews table to itself\n","crews_self_merged = crews.merge(crews, on = \"id\", suffixes = [\"_dir\", \"_crew\"])"],"metadata":{"id":"48xnx_FY-1iL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2/3\n","\n","Create a Boolean index, named boolean_filter, that selects rows from the left table with the job of 'Director' and avoids rows with the job of 'Director' in the right table.\n","\n","Hint\n","\n","When checking the text value of the job_dir or job_crew columns, the text must be put into quotes."],"metadata":{"id":"B_zOkEXc-6ck"}},{"cell_type":"code","source":["# Merge the crews table to itself\n","crews_self_merged = crews.merge(crews, on='id', how='inner',\n","                                suffixes=('_dir','_crew'))\n","\n","# Create a Boolean index to select the appropriate\n","boolean_filter = ((crews_self_merged['job_dir'] == \"Director\") &\n","     (crews_self_merged['job_crew'] != \"Director\"))\n","direct_crews = crews_self_merged[boolean_filter]"],"metadata":{"id":"VczudUdo-9L_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"Y0lsgpRu_c-d"}},{"cell_type":"markdown","source":["# 3/3\n","\n","Use the .head() method to print the first few rows of direct_crews."],"metadata":{"id":"_XUcOt0T_Bgu"}},{"cell_type":"code","source":["# Merge the crews table to itself\n","crews_self_merged = crews.merge(crews, on='id', how='inner',\n","                                suffixes=('_dir','_crew'))\n","\n","# Create a boolean index to select the appropriate rows\n","boolean_filter = ((crews_self_merged['job_dir'] == 'Director') &\n","                  (crews_self_merged['job_crew'] != 'Director'))\n","direct_crews = crews_self_merged[boolean_filter]\n","\n","# Print the first few rows of direct_crews\n","print(direct_crews.head())"],"metadata":{"id":"sbwtVOE9_G11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","            id   job_dir       name_dir        job_crew          name_crew\n","    156  19995  Director  James Cameron          Editor  Stephen E. Rivkin\n","    157  19995  Director  James Cameron  Sound Designer  Christopher Boyes\n","    158  19995  Director  James Cameron         Casting          Mali Finn\n","    160  19995  Director  James Cameron          Writer      James Cameron\n","    161  19995  Director  James Cameron    Set Designer    Richard F. Mays"],"metadata":{"id":"xGLJuAum_KCu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great job! By merging the table to itself, you compared the value of the __director__ from the jobs column to other values from the jobs column. With the output, you can quickly see different movie directors and the people they worked with in the same movie."],"metadata":{"id":"4VGtyY_2_L_M"}},{"cell_type":"markdown","source":["# Merging on indexes\n","\n","# Index merge for movie ratings\n","\n","To practice merging on indexes, you will merge movies and a table called ratings that holds info about movie ratings. Make sure your merge returns all of the rows from the movies table and not all the rows of ratings table need to be included in the result.\n","\n","The movies and ratings tables have been loaded for you.\n","\n","## Instructions\n","\n","Merge movies and ratings on the index and save to a variable called movies_ratings, ensuring that all of the rows from the movies table are returned."],"metadata":{"id":"T0TRbXMd_fWA"}},{"cell_type":"code","source":["# Merge to the movies table the ratings table on the index\n","movies_ratings = movies.merge(ratings, on = \"id\")\n","\n","# Print the first few rows of movies_ratings\n","print(movies_ratings.head())"],"metadata":{"id":"QnOriv1j_pXO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","                          title  popularity release_date  vote_average  vote_count\n","    id\n","    257            Oliver Twist      20.416   2005-09-23           6.7       274.0\n","    14290  Better Luck Tomorrow       3.877   2002-01-12           6.5        27.0\n","    38365             Grown Ups      38.864   2010-06-24           6.0      1705.0\n","    9672               Infamous       3.681   2006-11-16           6.4        60.0\n","    12819       Alpha and Omega      12.301   2010-09-17           5.3       124.0"],"metadata":{"id":"Hgbv2c_j_swJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Good work! Merging on indexes is just like merging on columns, so if you need to merge based on indexes, there's no need to turn the indexes into columns first."],"metadata":{"id":"FosveBhi_u4Y"}},{"cell_type":"markdown","source":["# Do sequels earn more?\n","It is time to put together many of the aspects that you have learned in this chapter. In this exercise, you'll find out which movie sequels earned the most compared to the original movie. To answer this question, you will merge a modified version of the sequels and financials tables where their index is the movie ID. You will need to choose a merge type that will return all of the rows from the sequels table and not all the rows of financials table need to be included in the result. From there, you will join the resulting table to itself so that you can compare the revenue values of the original movie to the sequel. Next, you will calculate the difference between the two revenues and sort the resulting dataset.\n","\n","The sequels and financials tables have been provided.\n","\n","Instructions 1/4\n","\n","With the sequels table on the left, merge to it the financials table on index named id, ensuring that all the rows from the sequels are returned and some rows from the other table may not be returned, Save the results to sequels_fin."],"metadata":{"id":"W-3m2Su4Ahbl"}},{"cell_type":"code","source":["# Merge sequels and financials on index id\n","sequels_fin = sequels.merge(financials, on = \"id\", how =\"left\")\n","print(sequels_fin)"],"metadata":{"id":"GBmdyZeBAoId"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2/4\n","Merge the sequels_fin table to itself with an inner join, where the left and right tables merge on sequel and id respectively with suffixes equal to ('_org','_seq'), saving to orig_seq."],"metadata":{"id":"KS5V0tsJAsKt"}},{"cell_type":"code","source":["# Merge sequels and financials on index id\n","sequels_fin = sequels.merge(financials, on='id', how='left')\n","\n","# Self merge with suffixes as inner join with left on sequel and right on id\n","orig_seq = sequels_fin.merge(sequels_fin, how=\"inner\", left_on=\"sequel\",\n","                             right_on=\"id\", right_index=True,\n","                             suffixes=[\"_org\",\"_seq\" ])\n","\n","# Add calculation to subtract revenue_org from revenue_seq\n","orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n"],"metadata":{"id":"wOESEgoOAutd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","                                       title sequel     budget    revenue\n","    id\n","    19995                             Avatar   <NA>  2.370e+08  2.788e+09\n","    862                            Toy Story    863  3.000e+07  3.736e+08\n","    863                          Toy Story 2  10193  9.000e+07  4.974e+08\n","    597                              Titanic   <NA>  2.000e+08  1.845e+09\n","    24428                       The Avengers   <NA>  2.200e+08  1.520e+09\n","    ...                                  ...    ...        ...        ...\n","    133931                          Zambezia   <NA>        NaN        NaN\n","    309503                            Zipper   <NA>        NaN        NaN\n","    34592   ZMD: Zombies of Mass Destruction   <NA>        NaN        NaN\n","    206213                     Zombie Hunter   <NA>        NaN        NaN\n","    185567                              Zulu   <NA>        NaN        NaN\n","\n","    [4803 rows x 4 columns]"],"metadata":{"id":"_5KlWCgQAx-k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3/4\n","\n","Select the title_org, title_seq, and diff columns of orig_seq and save this as titles_diff."],"metadata":{"id":"yyZJHHA4A8_d"}},{"cell_type":"code","source":["# Merge sequels and financials on index id\n","sequels_fin = sequels.merge(financials, on='id', how='left')\n","\n","# Self merge with suffixes as inner join with left on sequel and right on id\n","orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel',\n","                             right_on='id', right_index=True,\n","                             suffixes=('_org','_seq'))\n","\n","# Add calculation to subtract revenue_org from revenue_seq\n","orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n","\n","# Select the title_org, title_seq, and diff\n","titles_diff = orig_seq[[\"title_org\", \"title_seq\", \"diff\"]]\n","print(titles_diff)"],"metadata":{"id":"HQZ0YlLhBIid"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","                                                   title_org                                      title_seq       diff\n","    id\n","    862                                            Toy Story                                    Toy Story 2  1.238e+08\n","    863                                          Toy Story 2                                    Toy Story 3  5.696e+08\n","    675            Harry Potter and the Order of the Phoenix         Harry Potter and the Half-Blood Prince -4.254e+06\n","    121                The Lord of the Rings: The Two Towers  The Lord of the Rings: The Return of the King  1.926e+08\n","    120    The Lord of the Rings: The Fellowship of the Ring          The Lord of the Rings: The Two Towers  5.492e+07\n","    ...                                                  ...                                            ...        ...\n","    76                                        Before Sunrise                                  Before Sunset  1.046e+07\n","    2292                                              Clerks                                      Clerks II  2.374e+07\n","    9367                                         El Mariachi                                      Desperado  2.336e+07\n","    8374                                 The Boondock Saints         The Boondock Saints II: All Saints Day  1.060e+07\n","    16186                         Diary of a Mad Black Woman                         Madea's Family Reunion        NaN\n","\n","    [90 rows x 3 columns]"],"metadata":{"id":"B0Bg_-O8BOsu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4/4\n","\n","Sort by titles_diff by diff in descending order and print the first few rows."],"metadata":{"id":"349EyRujBRWu"}},{"cell_type":"code","source":["# Merge sequels and financials on index id\n","sequels_fin = sequels.merge(financials, on='id', how='left')\n","\n","# Self merge with suffixes as inner join with left on sequel and right on id\n","orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel',\n","                             right_on='id', right_index=True,\n","                             suffixes=('_org','_seq'))\n","\n","# Add calculation to subtract revenue_org from revenue_seq\n","orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n","\n","# Select the title_org, title_seq, and diff\n","titles_diff = orig_seq[['title_org','title_seq','diff']]\n","\n","# Print the first rows of the sorted titles_diff\n","print(titles_diff.sort_values(\"diff\", ascending = False).head())"],"metadata":{"id":"ovDiirt2Bd5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","                                                   title_org                                      title_seq       diff\n","    id\n","    862                                            Toy Story                                    Toy Story 2  1.238e+08\n","    863                                          Toy Story 2                                    Toy Story 3  5.696e+08\n","    675            Harry Potter and the Order of the Phoenix         Harry Potter and the Half-Blood Prince -4.254e+06\n","    121                The Lord of the Rings: The Two Towers  The Lord of the Rings: The Return of the King  1.926e+08\n","    120    The Lord of the Rings: The Fellowship of the Ring          The Lord of the Rings: The Two Towers  5.492e+07\n","    ...                                                  ...                                            ...        ...\n","    76                                        Before Sunrise                                  Before Sunset  1.046e+07\n","    2292                                              Clerks                                      Clerks II  2.374e+07\n","    9367                                         El Mariachi                                      Desperado  2.336e+07\n","    8374                                 The Boondock Saints         The Boondock Saints II: All Saints Day  1.060e+07\n","    16186                         Diary of a Mad Black Woman                         Madea's Family Reunion        NaN\n","\n","    [90 rows x 3 columns]\n","\n","<script.py> output:\n","                   title_org        title_seq       diff\n","    id\n","    331    Jurassic Park III   Jurassic World  1.145e+09\n","    272        Batman Begins  The Dark Knight  6.303e+08\n","    10138         Iron Man 2       Iron Man 3  5.915e+08\n","    863          Toy Story 2      Toy Story 3  5.696e+08\n","    10764  Quantum of Solace          Skyfall  5.225e+08"],"metadata":{"id":"dqznT1ajBgae"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Amazing, that was great work! To complete this exercise, you needed to merge tables on their index and merge another table to itself. After the calculations were added and sub-select specific columns, the data was sorted. You found out that Jurassic World had one of the highest of all, improvement in revenue compared to the original movie."],"metadata":{"id":"HV9r6s01BvE-"}},{"cell_type":"markdown","source":["# Chapter 3: Advanced Merging and Concatenating\n","\n","In this chapter, you’ll leverage powerful filtering techniques, including semi-joins and anti-joins. You’ll also learn how to glue DataFrames by vertically combining and using the pandas.concat function to create new datasets. Finally, because data is rarely clean, you’ll also learn how to validate your newly combined data structures."],"metadata":{"id":"0DHL5v21BzmA"}},{"cell_type":"markdown","source":["# Filtering joins\n","# Performing an anti join\n","In our music streaming company dataset, each customer is assigned an employee representative to assist them. In this exercise, filter the employee table by a table of top customers, returning only those employees who are not assigned to a customer. The results should resemble the results of an anti join. The company's leadership will assign these employees additional training so that they can work with high valued customers.\n","\n","The top_cust and employees tables have been provided for you.\n","\n","## Instructions 1/3\n","\n","Merge employees and top_cust with a left join, setting indicator argument to True. Save the result to empl_cust."],"metadata":{"id":"3DzMVn2hB8QE"}},{"cell_type":"code","source":["# Merge employees and top_cust\n","empl_cust = employees.merge(top_cust, on=\"srid\",\n","                            how=\"left\", indicator=True)\n"],"metadata":{"id":"q9ZhzhovCJ6z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2/3\n","\n","Select the srid column of empl_cust and the rows where _merge is 'left_only'. Save the result to srid_list."],"metadata":{"id":"XFkUR5eTCNkQ"}},{"cell_type":"code","source":["# Merge employees and top_cust\n","empl_cust = employees.merge(top_cust, on='srid',\n","                            how='left', indicator=True)\n","\n","# Select the srid column where _merge is left_only\n","srid_list = empl_cust.loc[empl_cust[\"_merge\"] == 'left_only', \"srid\"]"],"metadata":{"id":"uep6OoBFCStI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3/3\n","Subset the employees table and select those rows where the srid is in the variable srid_list and print the results."],"metadata":{"id":"6js7redfCVsu"}},{"cell_type":"code","source":["# Merge employees and top_cust\n","empl_cust = employees.merge(top_cust, on='srid',\n","                                 how='left', indicator=True)\n","\n","# Select the srid column where _merge is left_only\n","srid_list = empl_cust.loc[empl_cust['_merge'] == 'left_only', 'srid']\n","\n","# Get employees not working with top customers\n","print(employees[employees[\"srid\"].isin(srid_list)])"],"metadata":{"id":"T8Jib5z6CaL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","       srid     lname    fname            title  hire_date                    email\n","    0     1     Adams   Andrew  General Manager 2002-08-14   andrew@chinookcorp.com\n","    1     2   Edwards    Nancy    Sales Manager 2002-05-01    nancy@chinookcorp.com\n","    5     6  Mitchell  Michael       IT Manager 2003-10-17  michael@chinookcorp.com\n","    6     7      King   Robert         IT Staff 2004-01-02   robert@chinookcorp.com\n","    7     8  Callahan    Laura         IT Staff 2004-03-04    laura@chinookcorp.com"],"metadata":{"id":"UiUPEb-1CdlH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"G4iME28ECgs3"}},{"cell_type":"markdown","source":["Success! You performed an anti join by first merging the tables with a left join, selecting the ID of those employees who did not support a top customer, and then subsetting the original employee's table. From that, we can see that there are five employees not supporting top customers. Anti joins are a powerful tool to filter a main table (i.e. employees) by another (i.e. customers)."],"metadata":{"id":"eJpSZ_PXCiaW"}},{"cell_type":"markdown","source":["# Performing a semi join\n","Some of the tracks that have generated the most significant amount of revenue are from TV-shows or are other non-musical audio. You have been given a table of invoices that include top revenue-generating items. Additionally, you have a table of non-musical tracks from the streaming service. In this exercise, you'll use a semi join to find the top revenue-generating non-musical tracks..\n","\n","The tables non_mus_tcks, top_invoices, and genres have been loaded for you.\n","\n","## Instructions\n","\n","Merge non_mus_tcks and top_invoices on tid using an inner join. Save the result as tracks_invoices.\n","Use .isin() to subset the rows of non_mus_tck where tid is in the tid column of tracks_invoices. Save the result as top_tracks.\n","Group top_tracks by gid and count the tid rows. Save the result to cnt_by_gid.\n","Merge cnt_by_gid with the genres table on gid and print the result.\n"],"metadata":{"id":"yoLZkNwvCnyo"}},{"cell_type":"code","source":["# Merge the non_mus_tck and top_invoices tables on tid\n","tracks_invoices = non_mus_tcks.merge(top_invoices, on = \"tid\")\n","\n","# Use .isin() to subset non_mus_tcks to rows with tid in tracks_invoices\n","top_tracks = non_mus_tcks[non_mus_tcks['tid'].isin(tracks_invoices[\"tid\"])]\n","\n","# Group the top_tracks by gid and count the tid rows\n","cnt_by_gid = top_tracks.groupby(['gid'], as_index=False).agg({'tid':\"count\"})\n","\n","# Merge the genres table to cnt_by_gid on gid and print\n","print(cnt_by_gid.merge(genres, on = \"gid\"))"],"metadata":{"id":"DCHA4EfKCrhS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","       gid  tid      name\n","    0   19    4  TV Shows\n","    1   21    2     Drama\n","    2   22    1    Comedy"],"metadata":{"id":"cKbT49yXCvfq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nice job! In this exercise, you replicated a semi join to filter the table of tracks by the table of invoice items to find the top revenue non-musical tracks. With some additional data manipulation, you discovered that _'TV-shows'_ is the non-musical genre that has the most top revenue-generating tracks. Now that you've done both semi- and anti joins, it's time to move to the next topic."],"metadata":{"id":"a5ffAg65CyPP"}},{"cell_type":"markdown","source":["# Concatenate DataFrames together vertically\n","\n","Concatenation basics\n","You have been given a few tables of data with musical track info for different albums from the metal band, Metallica. The track info comes from their Ride The Lightning, Master Of Puppets, and St. Anger albums. Try various features of the .concat() method by concatenating the tables vertically together in different ways.\n","\n","The tables tracks_master, tracks_ride, and tracks_st have loaded for you.\n","\n","## Instructions 1/3\n","\n","Concatenate tracks_master, tracks_ride, and tracks_st, in that order, setting sort to True.\n","## 2/3\n","Concatenate tracks_master, tracks_ride, and tracks_st, where the index goes from 0 to n-1.\n","## 3/3\n","Concatenate tracks_master, tracks_ride, and tracks_st, showing only columns that are in all tables."],"metadata":{"id":"h_T3fJ9ZC4Ju"}},{"cell_type":"code","source":["# Concatenate the tracks\n","tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st],\n","                               sort=True)\n","print(tracks_from_albums)"],"metadata":{"id":"6dv4KxiVDGT3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","       aid             composer  gid  mtid                     name   tid  u_price\n","    0  152  J.Hetfield/L.Ulrich    3     1                  Battery  1853     0.99\n","    1  152            K.Hammett    3     1        Master Of Puppets  1854     0.99\n","    4  152  J.Hetfield/L.Ulrich    3     1        Disposable Heroes  1857     0.99\n","    0  154                  NaN    3     1     Fight Fire With Fire  1874     0.99\n","    1  154                  NaN    3     1       Ride The Lightning  1875     0.99\n","    2  154                  NaN    3     1  For Whom The Bell Tolls  1876     0.99\n","    3  154                  NaN    3     1            Fade To Black  1877     0.99\n","    4  154                  NaN    3     1        Trapped Under Ice  1878     0.99\n","    0  155                  NaN    3     1                  Frantic  1882     0.99\n","    1  155                  NaN    3     1                St. Anger  1883     0.99\n","    2  155                  NaN    3     1     Some Kind Of Monster  1884     0.99\n","    3  155                  NaN    3     1             Dirty Window  1885     0.99\n","    4  155                  NaN    3     1            Invisible Kid  1886     0.99"],"metadata":{"id":"hGFfhVEtDKK3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Concatenate the tracks so the index goes from 0 to n-1\n","tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st],\n","                               ignore_index = True,\n","                               sort=True)\n","print(tracks_from_albums)"],"metadata":{"id":"9xUunuvFDNgS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","        aid             composer  gid  mtid                     name   tid  u_price\n","    0   152  J.Hetfield/L.Ulrich    3     1                  Battery  1853     0.99\n","    1   152            K.Hammett    3     1        Master Of Puppets  1854     0.99\n","    2   152  J.Hetfield/L.Ulrich    3     1        Disposable Heroes  1857     0.99\n","    3   154                  NaN    3     1     Fight Fire With Fire  1874     0.99\n","    4   154                  NaN    3     1       Ride The Lightning  1875     0.99\n","    5   154                  NaN    3     1  For Whom The Bell Tolls  1876     0.99\n","    6   154                  NaN    3     1            Fade To Black  1877     0.99\n","    7   154                  NaN    3     1        Trapped Under Ice  1878     0.99\n","    8   155                  NaN    3     1                  Frantic  1882     0.99\n","    9   155                  NaN    3     1                St. Anger  1883     0.99\n","    10  155                  NaN    3     1     Some Kind Of Monster  1884     0.99\n","    11  155                  NaN    3     1             Dirty Window  1885     0.99\n","    12  155                  NaN    3     1            Invisible Kid  1886     0.99"],"metadata":{"id":"n3PG5NLsDSg4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Concatenate the tracks, show only columns names that are in all tables\n","tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st],\n","                               join = \"inner\",\n","                               sort=True)\n","print(tracks_from_albums)"],"metadata":{"id":"i5dQz95bDgKp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","        aid             composer  gid  mtid                     name   tid  u_price\n","    0   152  J.Hetfield/L.Ulrich    3     1                  Battery  1853     0.99\n","    1   152            K.Hammett    3     1        Master Of Puppets  1854     0.99\n","    2   152  J.Hetfield/L.Ulrich    3     1        Disposable Heroes  1857     0.99\n","    3   154                  NaN    3     1     Fight Fire With Fire  1874     0.99\n","    4   154                  NaN    3     1       Ride The Lightning  1875     0.99\n","    5   154                  NaN    3     1  For Whom The Bell Tolls  1876     0.99\n","    6   154                  NaN    3     1            Fade To Black  1877     0.99\n","    7   154                  NaN    3     1        Trapped Under Ice  1878     0.99\n","    8   155                  NaN    3     1                  Frantic  1882     0.99\n","    9   155                  NaN    3     1                St. Anger  1883     0.99\n","    10  155                  NaN    3     1     Some Kind Of Monster  1884     0.99\n","    11  155                  NaN    3     1             Dirty Window  1885     0.99\n","    12  155                  NaN    3     1            Invisible Kid  1886     0.99\n","\n","<script.py> output:\n","       aid  gid  mtid                     name   tid  u_price\n","    0  152    3     1                  Battery  1853     0.99\n","    1  152    3     1        Master Of Puppets  1854     0.99\n","    4  152    3     1        Disposable Heroes  1857     0.99\n","    0  154    3     1     Fight Fire With Fire  1874     0.99\n","    1  154    3     1       Ride The Lightning  1875     0.99\n","    2  154    3     1  For Whom The Bell Tolls  1876     0.99\n","    3  154    3     1            Fade To Black  1877     0.99\n","    4  154    3     1        Trapped Under Ice  1878     0.99\n","    0  155    3     1                  Frantic  1882     0.99\n","    1  155    3     1                St. Anger  1883     0.99\n","    2  155    3     1     Some Kind Of Monster  1884     0.99\n","    3  155    3     1             Dirty Window  1885     0.99\n","    4  155    3     1            Invisible Kid  1886     0.99"],"metadata":{"id":"wXj8TpNoDkcP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great job! You've concatenated your first set of tables, adjusted the index, and altered the columns shown in the output. The .concat() method is a very flexible tool that is useful for combining data into a new dataset."],"metadata":{"id":"tKlL_VVPDngv"}},{"cell_type":"markdown","source":["# Concatenating with keys\n","The leadership of the music streaming company has come to you and asked you for assistance in analyzing sales for a recent business quarter. They would like to know which month in the quarter saw the highest average invoice total. You have been given three tables with invoice data named inv_jul, inv_aug, and inv_sep. Concatenate these tables into one to create a graph of the average monthly invoice total.\n","\n","# Instructions\n","\n","Concatenate the three tables together vertically in order with the oldest month first, adding '7Jul', '8Aug', and '9Sep' as keys for their respective months, and save to variable avg_inv_by_month.\n","Use the .agg() method to find the average of the total column from the grouped invoices.\n","Create a bar chart of avg_inv_by_month."],"metadata":{"id":"3vB4DHe3DriX"}},{"cell_type":"code","source":["# Concatenate the tables and add keys\n","inv_jul_thr_sep = pd.concat([inv_jul, inv_aug, inv_sep],\n","                            keys=[\"7Jul\", \"8Aug\", \"9Sep\"])\n","\n","# Group the invoices by the index keys and find avg of the total column\n","avg_inv_by_month = inv_jul_thr_sep.groupby(level=0).agg({\"total\":\"mean\"})\n","\n","# Bar plot of avg_inv_by_month\n","avg_inv_by_month.plot(kind='bar')\n","plt.show()\n"],"metadata":{"id":"6Zm1wq0MDwKh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Way to come through! There are many ways to write code for this task. However, concatenating the tables with a key provides a hierarchical index that can be used for grouping. Once grouped, you can average the groups and create plots. You were able to find out that September had the highest average invoice total."],"metadata":{"id":"fglopSHjEAz_"}},{"cell_type":"markdown","source":["# Verifying integrity\n","\n","# Validating a merge\n","You have been given 2 tables, artists, and albums. Use the IPython shell to merge them using artists.merge(albums, on='artid').head(). Adjust the validate argument to answer which statement is False.\n","\n","## Instructions\n","\n","Possible answers\n","\n","\n","You can use 'many_to_many' without an error, since there is a duplicate key in one of the tables.\n","\n","You can use 'one_to_many' without error, since there is a duplicate key in the right table.\n","\n","**You can use 'many_to_one' without an error, since there is a duplicate key in the left table.**"],"metadata":{"id":"c8mXxqBhEF1B"}},{"cell_type":"markdown","source":["That's correct! This statement is false. There is a duplicate value in the artid column in the albums table, which is the right table in this merge. Therefore, setting validate equal to 'many_to_one' or 'one_to_one' will raise an error, making this statement false."],"metadata":{"id":"zUIrXpmdEVrh"}},{"cell_type":"markdown","source":["# Concatenate and merge to find common songs\n","The senior leadership of the streaming service is requesting your help again. You are given the historical files for a popular playlist in the classical music genre in 2018 and 2019. Additionally, you are given a similar set of files for the most popular pop music genre playlist on the streaming service in 2018 and 2019. Your goal is to concatenate the respective files to make a large classical playlist table and overall popular music table. Then filter the classical music table using a semi join to return only the most popular classical music tracks.\n","\n","The tables classic_18, classic_19, and pop_18, pop_19 have been loaded for you. Additionally, pandas has been loaded as pd.\n","\n","## Instructions 1/2\n","\n","Concatenate the classic_18 and classic_19 tables vertically where the index goes from 0 to n-1, and save to classic_18_19.\n","Concatenate the pop_18 and pop_19 tables vertically where the index goes from 0 to n-1, and save to pop_18_19.\n","\n","Hint\n","\n","You can make the index goo from 0 to n-1 using the ignore_index argument."],"metadata":{"id":"UWaWsM6hEbnb"}},{"cell_type":"code","source":["# Concatenate the classic tables vertically\n","classic_18_19 = pd.concat([classic_18, classic_19], ignore_index=True)\n","\n","# Concatenate the pop tables vertically\n","pop_18_19 = pd.concat([pop_18, pop_19], ignore_index=True)"],"metadata":{"id":"o_qpQTReEhXZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2/2\n","\n","With classic_18_19 on the left, merge it with pop_18_19 on tid using an inner join.\n","Use .isin() to filter classic_18_19 where tid is in classic_pop.\n","\n","Hint\n","\n","When using the .isin() method, make sure you are checking for the pid in the classic_pop table."],"metadata":{"id":"_EocoWQ9EMO0"}},{"cell_type":"code","source":["# Concatenate the classic tables vertically\n","classic_18_19 = pd.concat([classic_18, classic_19], ignore_index=True)\n","\n","# Concatenate the pop tables vertically\n","pop_18_19 = pd.concat([pop_18, pop_19], ignore_index=True)\n","\n","# Merge classic_18_19 with pop_18_19\n","classic_pop = classic_18_19.merge(pop_18_19, on=\"tid\")\n","\n","# Using .isin(), filter classic_18_19 rows where tid is in classic_pop\n","popular_classic = classic_18_19[classic_18_19[\"tid\"].isin(classic_pop[\"tid\"])]\n","\n","# Print popular chart\n","print(popular_classic)"],"metadata":{"id":"RZ0G6nAPEsKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","        pid   tid\n","    3    12  3479\n","    10   12  3439\n","    21   12  3445\n","    23   12  3449\n","    48   12  3437\n","    50   12  3435"],"metadata":{"id":"dK9VlJ_FEuqo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Excellent work! In this exercise, you demonstrated many of the concepts discussed in this chapter, including concatenation, and semi joins. You now have experience combining data vertically and using semi- and anti joins. Time to move on to the next chapter!"],"metadata":{"id":"V41PmRpJExWX"}},{"cell_type":"markdown","source":["# **Chapter 4:** Merging Ordered and Time-Series Data\n","\n","In this final chapter, you’ll step up a gear and learn to apply pandas' specialized methods for merging time-series and ordered data together with real-world financial and economic data from the city of Chicago. You’ll also learn how to query resulting tables using a SQL-style format, and unpivot data using the melt method.\n","\n","# Using merge_ordered()\n","\n","## Correlation between GDP and S&P500\n","\n","In this exercise, you want to analyze stock returns from the S&P 500. You believe there may be a relationship between the returns of the S&P 500 and the GDP of the US. Merge the different datasets together to compute the correlation.\n","\n","Two tables have been provided for you, named sp500, and gdp. As always, pandas has been imported for you as pd.\n","\n","## Instructions 1/3\n","\n","Use merge_ordered() to merge gdp and sp500 using a left join on year and date. Save the results as gdp_sp500.\n","Print gdp_sp500 and look at the returns for the year 2018."],"metadata":{"id":"vronDvcLE4sz"}},{"cell_type":"code","source":["# Use merge_ordered() to merge gdp and sp500 on year and date\n","gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on=\"year\", right_on=\"date\",\n","                             how=\"left\")\n","\n","# Print gdp_sp500\n","print(gdp_sp500)"],"metadata":{"id":"DkKCm-a3FSBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","      country code  year        gdp    date  returns\n","    0          USA  2010  1.499e+13  2010.0    12.78\n","    1          USA  2011  1.554e+13  2011.0     0.00\n","    2          USA  2012  1.620e+13  2012.0    13.41\n","    3          USA  2012  1.620e+13  2012.0    13.41\n","    4          USA  2013  1.678e+13  2013.0    29.60\n","    5          USA  2014  1.752e+13  2014.0    11.39\n","    6          USA  2015  1.822e+13  2015.0    -0.73\n","    7          USA  2016  1.871e+13  2016.0     9.54\n","    8          USA  2017  1.949e+13  2017.0    19.42\n","    9          USA  2018  2.049e+13     NaN      NaN"],"metadata":{"id":"8bA7LOn3FXZP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2/3\n","\n","Use merge_ordered(), again similar to before, to merge gdp and sp500 use the function's ability to interpolate missing data to forward fill the missing value for returns, assigning this table to the variable gdp_sp500.\n","\n","Hint\n","\n","To interpolate the missing value, use the fill_method argument of the merge_ordered() function."],"metadata":{"id":"ijP-QtK1FY5v"}},{"cell_type":"code","source":["# Use merge_ordered() to merge gdp and sp500, interpolate missing value\n","gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on=\"year\", right_on=\"date\", how=\"left\", fill_method = \"ffill\")\n","\n","\n","# Print gdp_sp500\n","print (gdp_sp500)"],"metadata":{"id":"8CzzkiaTFgpA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","      country code  year        gdp  date  returns\n","    0          USA  2010  1.499e+13  2010    12.78\n","    1          USA  2011  1.554e+13  2011     0.00\n","    2          USA  2012  1.620e+13  2012    13.41\n","    3          USA  2012  1.620e+13  2012    13.41\n","    4          USA  2013  1.678e+13  2013    29.60\n","    5          USA  2014  1.752e+13  2014    11.39\n","    6          USA  2015  1.822e+13  2015    -0.73\n","    7          USA  2016  1.871e+13  2016     9.54\n","    8          USA  2017  1.949e+13  2017    19.42\n","    9          USA  2018  2.049e+13  2017    19.42"],"metadata":{"id":"iCl9ME_aFk7C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3/3\n","\n","Subset the gdp_sp500 table, select the gdp and returns columns, and save as gdp_returns.\n","Print the correlation matrix of the gdp_returns table using the .corr() method."],"metadata":{"id":"RjvnA1O1FoOR"}},{"cell_type":"code","source":["# Use merge_ordered() to merge gdp and sp500, interpolate missing value\n","gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date',\n","                             how='left',  fill_method='ffill')\n","\n","# Subset the gdp and returns columns\n","gdp_returns = gdp_sp500[[\"gdp\", \"returns\"]]\n","\n","# Print gdp_returns correlation\n","print (gdp_returns.corr())"],"metadata":{"id":"yVeXNgllFtAS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","      country code  year        gdp  date  returns\n","    0          USA  2010  1.499e+13  2010    12.78\n","    1          USA  2011  1.554e+13  2011     0.00\n","    2          USA  2012  1.620e+13  2012    13.41\n","    3          USA  2012  1.620e+13  2012    13.41\n","    4          USA  2013  1.678e+13  2013    29.60\n","    5          USA  2014  1.752e+13  2014    11.39\n","    6          USA  2015  1.822e+13  2015    -0.73\n","    7          USA  2016  1.871e+13  2016     9.54\n","    8          USA  2017  1.949e+13  2017    19.42\n","    9          USA  2018  2.049e+13  2017    19.42\n","\n","<script.py> output:\n","               gdp  returns\n","    gdp      1.000    0.212\n","    returns  0.212    1.000"],"metadata":{"id":"0nYL8kPLFw_P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Awesome work! You can see the different aspects of merge_ordered() and how you might use it on data that can be ordered. By using this function, you were able to fill in the missing data from 2019. Finally, the correlation of 0.21 between the GDP and S&P500 is low to moderate at best. You may want to find another predictor if you plan to play in the stock market."],"metadata":{"id":"YzG_18fHFzAQ"}},{"cell_type":"markdown","source":["# Phillips curve using merge_ordered()\n","There is an economic theory developed by A. W. Phillips which states that inflation and unemployment have an inverse relationship. The theory claims that with economic growth comes inflation, which in turn should lead to more jobs and less unemployment.\n","\n","You will take two tables of data from the U.S. Bureau of Labor Statistics, containing unemployment and inflation data over different periods, and create a Phillips curve. The tables have different frequencies. One table has a data entry every six months, while the other has a data entry every month. You will need to use the entries where you have data within both tables.\n","\n","The tables unemployment and inflation have been loaded for you.\n","\n","## Instructions\n","\n","Use merge_ordered() to merge the inflation and unemployment tables on date with an inner join, and save the results as inflation_unemploy.\n","Print the inflation_unemploy variable.\n","Using inflation_unemploy, create a scatter plot with unemployment_rate on the horizontal axis and cpi (inflation) on the vertical axis."],"metadata":{"id":"ofVPtu08F2qT"}},{"cell_type":"code","source":["# Use merge_ordered() to merge inflation, unemployment with inner join\n","inflation_unemploy = pd.merge_ordered(inflation, unemployment, on=\"date\", how = \"inner\")\n","\n","# Print inflation_unemploy\n","print(inflation_unemploy)\n","\n","# Plot a scatter plot of unemployment_rate vs cpi of inflation_unemploy\n","inflation_unemploy.plot(x= \"unemployment_rate\",y = \"cpi\", kind = \"scatter\")\n","plt.show()\n"],"metadata":{"id":"RF45qnc4F6Yc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","             date      cpi     seriesid                  data_type  unemployment_rate\n","    0  2014-01-01  235.288  CUSR0000SA0  SEASONALLY ADJUSTED INDEX                6.7\n","    1  2014-06-01  237.231  CUSR0000SA0  SEASONALLY ADJUSTED INDEX                6.1\n","    2  2015-01-01  234.718  CUSR0000SA0  SEASONALLY ADJUSTED INDEX                5.6\n","    3  2015-06-01  237.684  CUSR0000SA0  SEASONALLY ADJUSTED INDEX                5.3\n","    4  2016-01-01  237.833  CUSR0000SA0  SEASONALLY ADJUSTED INDEX                5.0\n","    5  2016-06-01  240.167  CUSR0000SA0  SEASONALLY ADJUSTED INDEX                4.9\n","    6  2017-01-01  243.780  CUSR0000SA0  SEASONALLY ADJUSTED INDEX                4.7\n","    7  2017-06-01  244.182  CUSR0000SA0  SEASONALLY ADJUSTED INDEX                4.3\n","    8  2018-01-01  248.884  CUSR0000SA0  SEASONALLY ADJUSTED INDEX                4.1\n","    9  2018-06-01  251.134  CUSR0000SA0  SEASONALLY ADJUSTED INDEX                4.0"],"metadata":{"id":"jxUkgm2nF_0x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["reat work! You created a Phillips curve. There are critics of the curve, but what is more important in this example is that you were able to use entries where you had entries in both tables by using an inner join. You might ask why not use the default outer join and use forward fill to fill to estimate the missing variables. You might choose differently. In this case, instead of showing an estimated unemployment rate (which is a continually changing measure) for five periods, that data was dropped from the plot."],"metadata":{"id":"4wCW1KE3GDgo"}},{"cell_type":"markdown","source":["# merge_ordered() caution, multiple columns\n","When using merge_ordered() to merge on multiple columns, the order is important when you combine it with the forward fill feature. The function sorts the merge on columns in the order provided. In this exercise, we will merge GDP and population data from the World Bank for the Australia and Sweden, reversing the order of the merge on columns. The frequency of the series are different, the GDP values are quarterly, and the population is yearly. Use the forward fill feature to fill in the missing data. Depending on the order provided, the fill forward will use unintended data to fill in the missing values.\n","\n","The tables gdp and pop have been loaded.\n","\n","## Instructions 1/2\n","50 XP\n","Use merge_ordered() on gdp and pop, merging on columns date and country with the fill feature, save to ctry_date.\n","\n","## 2/2\n","Perform the same merge of gdp and pop, but join on country and date (reverse of step 1) with the fill feature, saving this as date_ctry."],"metadata":{"id":"uKOYdlyxGVMw"}},{"cell_type":"code","source":["# Merge gdp and pop on date and country with fill and notice rows 2 and 3\n","ctry_date = pd.merge_ordered(gdp, pop, on=[\"date\", \"country\"],\n","                             fill_method = 'ffill')\n","\n","# Print ctry_date\n","print(ctry_date)"],"metadata":{"id":"lvKXGFdWGcMJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","             date    country         gdp  series_code_x       pop series_code_y\n","    0  1990-01-01  Australia  158051.132  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    1  1990-01-01     Sweden   79837.846  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    2  1990-04-01  Australia  158263.582  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    3  1990-04-01     Sweden   80582.286  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    4  1990-07-01  Australia  157329.279  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    5  1990-07-01     Sweden   79974.360  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    6  1990-09-01  Australia  158240.678  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    7  1990-09-01     Sweden   80106.497  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    8  1991-01-01  Australia  156195.954  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    9  1991-01-01     Sweden   79524.242  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    10 1991-04-01  Australia  155989.033  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    11 1991-04-01     Sweden   79073.059  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    12 1991-07-01  Australia  156635.858  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    13 1991-07-01     Sweden   79084.770  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    14 1991-09-01  Australia  156744.057  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    15 1991-09-01     Sweden   79740.606  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    16 1992-01-01  Australia  157916.081  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    17 1992-01-01     Sweden   79390.922  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    18 1992-04-01  Australia  159047.827  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    19 1992-04-01     Sweden   79060.283  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    20 1992-07-01  Australia  160658.176  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    21 1992-07-01     Sweden   78904.605  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    22 1992-09-01  Australia  163960.221  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    23 1992-09-01     Sweden   76996.837  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    24 1993-01-01  Australia  165097.495  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    25 1993-01-01     Sweden   75783.588  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    26 1993-04-01  Australia  166027.059  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    27 1993-04-01     Sweden   76708.548  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    28 1993-07-01  Australia  166203.179  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    29 1993-07-01     Sweden   77662.018  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    30 1993-09-01  Australia  169279.348  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    31 1993-09-01     Sweden   77703.304  NYGDPMKTPSAKD   8718561   SP.POP.TOTL"],"metadata":{"id":"DvxVQ5dEGgtL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge gdp and pop on country and date with fill\n","date_ctry = pd.merge_ordered(gdp, pop, on=[\"country\", \"date\"], fill_method=\"ffill\")\n","\n","# Print date_ctry\n","print(date_ctry)"],"metadata":{"id":"6wIPfV83GmV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","             date    country         gdp  series_code_x       pop series_code_y\n","    0  1990-01-01  Australia  158051.132  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    1  1990-01-01     Sweden   79837.846  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    2  1990-04-01  Australia  158263.582  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    3  1990-04-01     Sweden   80582.286  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    4  1990-07-01  Australia  157329.279  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    5  1990-07-01     Sweden   79974.360  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    6  1990-09-01  Australia  158240.678  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    7  1990-09-01     Sweden   80106.497  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    8  1991-01-01  Australia  156195.954  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    9  1991-01-01     Sweden   79524.242  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    10 1991-04-01  Australia  155989.033  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    11 1991-04-01     Sweden   79073.059  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    12 1991-07-01  Australia  156635.858  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    13 1991-07-01     Sweden   79084.770  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    14 1991-09-01  Australia  156744.057  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    15 1991-09-01     Sweden   79740.606  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    16 1992-01-01  Australia  157916.081  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    17 1992-01-01     Sweden   79390.922  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    18 1992-04-01  Australia  159047.827  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    19 1992-04-01     Sweden   79060.283  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    20 1992-07-01  Australia  160658.176  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    21 1992-07-01     Sweden   78904.605  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    22 1992-09-01  Australia  163960.221  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    23 1992-09-01     Sweden   76996.837  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    24 1993-01-01  Australia  165097.495  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    25 1993-01-01     Sweden   75783.588  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    26 1993-04-01  Australia  166027.059  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    27 1993-04-01     Sweden   76708.548  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    28 1993-07-01  Australia  166203.179  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    29 1993-07-01     Sweden   77662.018  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    30 1993-09-01  Australia  169279.348  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    31 1993-09-01     Sweden   77703.304  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","\n","<script.py> output:\n","             date    country         gdp  series_code_x       pop series_code_y\n","    0  1990-01-01  Australia  158051.132  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    1  1990-04-01  Australia  158263.582  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    2  1990-07-01  Australia  157329.279  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    3  1990-09-01  Australia  158240.678  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    4  1991-01-01  Australia  156195.954  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    5  1991-04-01  Australia  155989.033  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    6  1991-07-01  Australia  156635.858  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    7  1991-09-01  Australia  156744.057  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    8  1992-01-01  Australia  157916.081  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    9  1992-04-01  Australia  159047.827  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    10 1992-07-01  Australia  160658.176  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    11 1992-09-01  Australia  163960.221  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    12 1993-01-01  Australia  165097.495  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    13 1993-04-01  Australia  166027.059  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    14 1993-07-01  Australia  166203.179  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    15 1993-09-01  Australia  169279.348  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    16 1990-01-01     Sweden   79837.846  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    17 1990-04-01     Sweden   80582.286  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    18 1990-07-01     Sweden   79974.360  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    19 1990-09-01     Sweden   80106.497  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    20 1991-01-01     Sweden   79524.242  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    21 1991-04-01     Sweden   79073.059  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    22 1991-07-01     Sweden   79084.770  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    23 1991-09-01     Sweden   79740.606  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    24 1992-01-01     Sweden   79390.922  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    25 1992-04-01     Sweden   79060.283  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    26 1992-07-01     Sweden   78904.605  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    27 1992-09-01     Sweden   76996.837  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    28 1993-01-01     Sweden   75783.588  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    29 1993-04-01     Sweden   76708.548  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    30 1993-07-01     Sweden   77662.018  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    31 1993-09-01     Sweden   77703.304  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","\n","<script.py> output:\n","             date    country         gdp  series_code_x       pop series_code_y\n","    0  1990-01-01  Australia  158051.132  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    1  1990-04-01  Australia  158263.582  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    2  1990-07-01  Australia  157329.279  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    3  1990-09-01  Australia  158240.678  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    4  1991-01-01  Australia  156195.954  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    5  1991-04-01  Australia  155989.033  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    6  1991-07-01  Australia  156635.858  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    7  1991-09-01  Australia  156744.057  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    8  1992-01-01  Australia  157916.081  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    9  1992-04-01  Australia  159047.827  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    10 1992-07-01  Australia  160658.176  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    11 1992-09-01  Australia  163960.221  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    12 1993-01-01  Australia  165097.495  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    13 1993-04-01  Australia  166027.059  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    14 1993-07-01  Australia  166203.179  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    15 1993-09-01  Australia  169279.348  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    16 1990-01-01     Sweden   79837.846  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    17 1990-04-01     Sweden   80582.286  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    18 1990-07-01     Sweden   79974.360  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    19 1990-09-01     Sweden   80106.497  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    20 1991-01-01     Sweden   79524.242  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    21 1991-04-01     Sweden   79073.059  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    22 1991-07-01     Sweden   79084.770  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    23 1991-09-01     Sweden   79740.606  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    24 1992-01-01     Sweden   79390.922  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    25 1992-04-01     Sweden   79060.283  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    26 1992-07-01     Sweden   78904.605  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    27 1992-09-01     Sweden   76996.837  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    28 1993-01-01     Sweden   75783.588  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    29 1993-04-01     Sweden   76708.548  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    30 1993-07-01     Sweden   77662.018  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    31 1993-09-01     Sweden   77703.304  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n"],"metadata":{"id":"qOKe1OwEGsOz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nice! When you merge on date first, the table is sorted by date then country. When forward fill is applied, Sweden's population value in January is used to fill in the missing values for both Australia and the Sweden for the remainder of the year. This is not what you want. The fill forward is using unintended data to fill in the missing values. However, when you merge on country first, the table is sorted by country then date, so the forward fill is applied appropriately in this situation."],"metadata":{"id":"JTdpAzZIGwf7"}},{"cell_type":"markdown","source":["# Using merge_asof()\n","\n","# Using merge_asof() to study stocks\n","You have a feed of stock market prices that you record. You attempt to track the price every five minutes. Still, due to some network latency, the prices you record are roughly every 5 minutes. You pull your price logs for three banks, JP Morgan (JPM), Wells Fargo (WFC), and Bank Of America (BAC). You want to know how the price change of the two other banks compare to JP Morgan. Therefore, you will need to merge these three logs into one table. Afterward, you will use the pandas .diff() method to compute the price change over time. Finally, plot the price changes so you can review your analysis.\n","\n","The three log files have been loaded for you as tables named jpm, wells, and bac.\n","\n","# Instructions\n","\n","Use merge_asof() to merge jpm (left table) and wells together on the date_time column, where the rows with the nearest times are matched, and with suffixes=('', '_wells'). Save to jpm_wells.\n","Use merge_asof() to merge jpm_wells (left table) and bac together on the date_time column, where the rows with the closest times are matched, and with suffixes=('_jpm', '_bac'). Save to jpm_wells_bac.\n","Using price_diffs, create a line plot of the close price of JPM, WFC, and BAC only."],"metadata":{"id":"aPwrAInWRdzE"}},{"cell_type":"code","source":["# Use merge_asof() to merge jpm and wells\n","jpm_wells = pd.merge_asof(jpm, wells, on=\"date_time\", suffixes=(\"\", \"_wells\"), direction='nearest')\n","\n","\n","# Use merge_asof() to merge jpm_wells and bac\n","jpm_wells_bac = pd.merge_asof(jpm_wells, bac, on='date_time', suffixes=('_jpm', '_bac'), direction='nearest')\n","\n","\n","# Compute price diff\n","price_diffs = jpm_wells_bac.diff()\n","\n","# Plot the price diff of the close of jpm, wells and bac only\n","price_diffs.plot(y=[\"close_jpm\", \"close_wells\", \"close_bac\"])\n","plt.show()"],"metadata":{"id":"FVsLq_ZMUrb-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fabulous! You can see that during this period, the price change for these bank stocks was roughly the same, although the price change for _JP Morgan_ was more variable. The critical point here is that the merge_asof() function is very useful in performing the fuzzy matching between the timestamps of all the tables."],"metadata":{"id":"zNO-RvoYU6NN"}},{"cell_type":"markdown","source":["# Using merge_asof() to create dataset\n","The merge_asof() function can be used to create datasets where you have a table of start and stop dates, and you want to use them to create a flag in another table. You have been given gdp, which is a table of quarterly GDP values of the US during the 1980s. Additionally, the table recession has been given to you. It holds the starting date of every US recession since 1980, and the date when the recession was declared to be over. Use merge_asof() to merge the tables and create a status flag if a quarter was during a recession. Finally, to check your work, plot the data in a bar chart.\n","\n","The tables gdp and recession have been loaded for you.\n","\n","## Instructions\n","\n","Using merge_asof(), merge gdp and recession on date, with gdp as the left table. Save to the variable gdp_recession.\n","Create a list using a list comprehension and a conditional expression, named is_recession, where for each row if the gdp_recession['econ_status'] value is equal to 'recession' then enter 'r' else 'g'.\n","Using gdp_recession, plot a bar chart of gdp versus date, setting the color argument equal to is_recession."],"metadata":{"id":"82WqnyqoVD_2"}},{"cell_type":"code","source":["# Merge gdp and recession on date using merge_asof()\n","gdp_recession = pd.merge_asof(gdp, recession, on=\"date\")\n","\n","# Create a list based on the row value of gdp_recession['econ_status']\n","is_recession = ['r' if s=='recession' else 'g' for s in gdp_recession['econ_status']]\n","\n","# Plot a bar chart of gdp_recession\n","gdp_recession.plot(kind=\"bar\", y=\"gdp\", x=\"date\", color=is_recession, rot=90)\n","plt.show()"],"metadata":{"id":"xlkafMTXVKgq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Terrific work! You can see from the chart that there were a number of quarters early in the 1980s where a recession was an issue. merge_asof() allowed you to quickly add a flag to the gdp dataset by matching between two different dates, in one line of code! If you were to perform the same task using subsetting, it would have taken a lot more code."],"metadata":{"id":"4yIpfkPNVZiW"}},{"cell_type":"markdown","source":["# Selecting data with .query()\n","\n","# Subsetting rows with .query()\n","\n","In this exercise, you will revisit GDP and population data for Australia and Sweden from the World Bank and expand on it using the .query() method. You'll merge the two tables and compute the GDP per capita. Afterwards, you'll use the .query() method to sub-select the rows and create a plot. Recall that you will need to merge on multiple columns in the proper order.\n","\n","The tables gdp and pop have been loaded for you.\n","\n","Instructions 1/4\n","\n","Use merge_ordered() on gdp and pop on columns country and date with the fill feature, save to gdp_pop and print."],"metadata":{"id":"lbckhgANVxNa"}},{"cell_type":"code","source":["# Merge gdp and pop on date and country with fill\n","gdp_pop = pd.merge_ordered(gdp, pop, on=[\"country\", \"date\"], fill_method=\"ffill\")\n","print(gdp_pop)"],"metadata":{"id":"ofGd3Gt7V__5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","             date    country         gdp  series_code_x       pop series_code_y\n","    0  1990-01-01  Australia  158051.132  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    1  1990-04-01  Australia  158263.582  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    2  1990-07-01  Australia  157329.279  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    3  1990-09-01  Australia  158240.678  NYGDPMKTPSAKD  17065100   SP.POP.TOTL\n","    4  1991-01-01  Australia  156195.954  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    5  1991-04-01  Australia  155989.033  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    6  1991-07-01  Australia  156635.858  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    7  1991-09-01  Australia  156744.057  NYGDPMKTPSAKD  17284000   SP.POP.TOTL\n","    8  1992-01-01  Australia  157916.081  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    9  1992-04-01  Australia  159047.827  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    10 1992-07-01  Australia  160658.176  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    11 1992-09-01  Australia  163960.221  NYGDPMKTPSAKD  17495000   SP.POP.TOTL\n","    12 1993-01-01  Australia  165097.495  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    13 1993-04-01  Australia  166027.059  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    14 1993-07-01  Australia  166203.179  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    15 1993-09-01  Australia  169279.348  NYGDPMKTPSAKD  17667000   SP.POP.TOTL\n","    16 1990-01-01     Sweden   79837.846  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    17 1990-04-01     Sweden   80582.286  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    18 1990-07-01     Sweden   79974.360  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    19 1990-09-01     Sweden   80106.497  NYGDPMKTPSAKD   8558835   SP.POP.TOTL\n","    20 1991-01-01     Sweden   79524.242  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    21 1991-04-01     Sweden   79073.059  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    22 1991-07-01     Sweden   79084.770  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    23 1991-09-01     Sweden   79740.606  NYGDPMKTPSAKD   8617375   SP.POP.TOTL\n","    24 1992-01-01     Sweden   79390.922  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    25 1992-04-01     Sweden   79060.283  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    26 1992-07-01     Sweden   78904.605  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    27 1992-09-01     Sweden   76996.837  NYGDPMKTPSAKD   8668067   SP.POP.TOTL\n","    28 1993-01-01     Sweden   75783.588  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    29 1993-04-01     Sweden   76708.548  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    30 1993-07-01     Sweden   77662.018  NYGDPMKTPSAKD   8718561   SP.POP.TOTL\n","    31 1993-09-01     Sweden   77703.304  NYGDPMKTPSAKD   8718561   SP.POP.TOTL"],"metadata":{"id":"taLI5GC7WD5u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2/4\n","Add a column named gdp_per_capita to gdp_pop that divides gdp by pop."],"metadata":{"id":"tcMBRdYVWGoB"}},{"cell_type":"code","source":["# Merge gdp and pop on date and country with fill\n","gdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n","\n","# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\n","gdp_pop[\"gdp_per_capita\"] = gdp_pop[\"gdp\"]/gdp_pop[\"pop\"]\n","print(gdp_pop)"],"metadata":{"id":"aI0b2WPUWMCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","             date    country         gdp  series_code_x       pop series_code_y  gdp_per_capita\n","    0  1990-01-01  Australia  158051.132  NYGDPMKTPSAKD  17065100   SP.POP.TOTL           0.009\n","    1  1990-04-01  Australia  158263.582  NYGDPMKTPSAKD  17065100   SP.POP.TOTL           0.009\n","    2  1990-07-01  Australia  157329.279  NYGDPMKTPSAKD  17065100   SP.POP.TOTL           0.009\n","    3  1990-09-01  Australia  158240.678  NYGDPMKTPSAKD  17065100   SP.POP.TOTL           0.009\n","    4  1991-01-01  Australia  156195.954  NYGDPMKTPSAKD  17284000   SP.POP.TOTL           0.009\n","    5  1991-04-01  Australia  155989.033  NYGDPMKTPSAKD  17284000   SP.POP.TOTL           0.009\n","    6  1991-07-01  Australia  156635.858  NYGDPMKTPSAKD  17284000   SP.POP.TOTL           0.009\n","    7  1991-09-01  Australia  156744.057  NYGDPMKTPSAKD  17284000   SP.POP.TOTL           0.009\n","    8  1992-01-01  Australia  157916.081  NYGDPMKTPSAKD  17495000   SP.POP.TOTL           0.009\n","    9  1992-04-01  Australia  159047.827  NYGDPMKTPSAKD  17495000   SP.POP.TOTL           0.009\n","    10 1992-07-01  Australia  160658.176  NYGDPMKTPSAKD  17495000   SP.POP.TOTL           0.009\n","    11 1992-09-01  Australia  163960.221  NYGDPMKTPSAKD  17495000   SP.POP.TOTL           0.009\n","    12 1993-01-01  Australia  165097.495  NYGDPMKTPSAKD  17667000   SP.POP.TOTL           0.009\n","    13 1993-04-01  Australia  166027.059  NYGDPMKTPSAKD  17667000   SP.POP.TOTL           0.009\n","    14 1993-07-01  Australia  166203.179  NYGDPMKTPSAKD  17667000   SP.POP.TOTL           0.009\n","    15 1993-09-01  Australia  169279.348  NYGDPMKTPSAKD  17667000   SP.POP.TOTL           0.010\n","    16 1990-01-01     Sweden   79837.846  NYGDPMKTPSAKD   8558835   SP.POP.TOTL           0.009\n","    17 1990-04-01     Sweden   80582.286  NYGDPMKTPSAKD   8558835   SP.POP.TOTL           0.009\n","    18 1990-07-01     Sweden   79974.360  NYGDPMKTPSAKD   8558835   SP.POP.TOTL           0.009\n","    19 1990-09-01     Sweden   80106.497  NYGDPMKTPSAKD   8558835   SP.POP.TOTL           0.009\n","    20 1991-01-01     Sweden   79524.242  NYGDPMKTPSAKD   8617375   SP.POP.TOTL           0.009\n","    21 1991-04-01     Sweden   79073.059  NYGDPMKTPSAKD   8617375   SP.POP.TOTL           0.009\n","    22 1991-07-01     Sweden   79084.770  NYGDPMKTPSAKD   8617375   SP.POP.TOTL           0.009\n","    23 1991-09-01     Sweden   79740.606  NYGDPMKTPSAKD   8617375   SP.POP.TOTL           0.009\n","    24 1992-01-01     Sweden   79390.922  NYGDPMKTPSAKD   8668067   SP.POP.TOTL           0.009\n","    25 1992-04-01     Sweden   79060.283  NYGDPMKTPSAKD   8668067   SP.POP.TOTL           0.009\n","    26 1992-07-01     Sweden   78904.605  NYGDPMKTPSAKD   8668067   SP.POP.TOTL           0.009\n","    27 1992-09-01     Sweden   76996.837  NYGDPMKTPSAKD   8668067   SP.POP.TOTL           0.009\n","    28 1993-01-01     Sweden   75783.588  NYGDPMKTPSAKD   8718561   SP.POP.TOTL           0.009\n","    29 1993-04-01     Sweden   76708.548  NYGDPMKTPSAKD   8718561   SP.POP.TOTL           0.009\n","    30 1993-07-01     Sweden   77662.018  NYGDPMKTPSAKD   8718561   SP.POP.TOTL           0.009\n","    31 1993-09-01     Sweden   77703.304  NYGDPMKTPSAKD   8718561   SP.POP.TOTL           0.009"],"metadata":{"id":"eUTentINWQVW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3/4\n","\n","Pivot gdp_pop so values='gdp_per_capita', index='date', and columns='country', save as gdp_pivot.\n"],"metadata":{"id":"2k2x2LnrWUAP"}},{"cell_type":"code","source":["# Merge gdp and pop on date and country with fill\n","gdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n","\n","# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\n","gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n","\n","# Pivot table of gdp_per_capita, where index is date and columns is country\n","gdp_pivot = gdp_pop.pivot_table('gdp_per_capita', 'date', 'country')\n","print(gdp_pivot)"],"metadata":{"id":"rb7fisBCWbOZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["<script.py> output:\n","    country     Australia  Sweden\n","    date\n","    1990-01-01      0.009   0.009\n","    1990-04-01      0.009   0.009\n","    1990-07-01      0.009   0.009\n","    1990-09-01      0.009   0.009\n","    1991-01-01      0.009   0.009\n","    1991-04-01      0.009   0.009\n","    1991-07-01      0.009   0.009\n","    1991-09-01      0.009   0.009\n","    1992-01-01      0.009   0.009\n","    1992-04-01      0.009   0.009\n","    1992-07-01      0.009   0.009\n","    1992-09-01      0.009   0.009\n","    1993-01-01      0.009   0.009\n","    1993-04-01      0.009   0.009\n","    1993-07-01      0.009   0.009\n","    1993-09-01      0.010   0.009"],"metadata":{"id":"_vQbfyLkWfSo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4/4\n","Use .query() to select rows from gdp_pivot where date is greater than equal to \"1991-01-01\". Save as recent_gdp_pop."],"metadata":{"id":"Ks8Bx0x3Whne"}},{"cell_type":"code","source":["# Merge gdp and pop on date and country with fill\n","gdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n","\n","# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\n","gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n","\n","# Pivot data so gdp_per_capita, where index is date and columns is country\n","gdp_pivot = gdp_pop.pivot_table('gdp_per_capita', 'date', 'country')\n","\n","# Select dates equal to or greater than 1991-01-01\n","recent_gdp_pop = gdp_pivot.query('date >= \"1991-01-01\"')\n","\n","# Plot recent_gdp_pop\n","recent_gdp_pop.plot(rot=90)\n","plt.show()"],"metadata":{"id":"AjeVjJUYWnjj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Amazing! You can see from the plot that the per capita GDP of Australia passed Sweden in 1992. By using the .query() method, you were able to select the appropriate rows easily. The .query() method is easy to read and straightforward."],"metadata":{"id":"L9fUW4gNWq8m"}},{"cell_type":"markdown","source":["# Reshaping data with .melt()\n","\n","# Using .melt() to reshape government data\n","The US Bureau of Labor Statistics (BLS) often provides data series in an easy-to-read format - it has a separate column for each month, and each year is a different row. Unfortunately, this wide format makes it difficult to plot this information over time. In this exercise, you will reshape a table of US unemployment rate data from the BLS into a form you can plot using .melt(). You will need to add a date column to the table and sort by it to plot the data correctly.\n","\n","The unemployment rate data has been loaded for you in a table called ur_wide. You are encouraged to view the table in the IPython shell before beginning the exercise.\n","\n","## Instructions\n","\n","Use .melt() to unpivot all of the columns of ur_wide except year and ensure that the columns with the months and values are named month and unempl_rate, respectively. Save the result as ur_tall.\n","Add a column to ur_tall named date which combines the year and month columns as year-month format into a larger string, and converts it to a date data type.\n","Sort ur_tall by date and save as ur_sorted.\n","Using ur_sorted, plot unempl_rate on the y-axis and date on the x-axis."],"metadata":{"id":"qrdPSbz7W_MI"}},{"cell_type":"code","source":["# unpivot everything besides the year column\n","ur_tall = ur_wide.melt(id_vars=\"year\", var_name = \"month\", value_name =\"unempl_rate\")\n","# Create a date column using the month and year columns of ur_tall\n","ur_tall['date'] = pd.to_datetime(ur_tall['year'] + '-' + ur_tall['month'])\n","\n","# Sort ur_tall by date in ascending order\n","ur_sorted = ur_tall.sort_values(\"date\")\n","\n","# Plot the unempl_rate by date\n","ur_sorted.plot(y = \"unempl_rate\", x = \"date\")\n","plt.show()"],"metadata":{"id":"KzVSted3XNGJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nice going! The plot shows a steady decrease in the unemployment rate with an increase near the end. This increase is likely the effect of the COVID-19 pandemic and its impact on shutting down most of the US economy. In general, data is often provided (_especially by governments_) in a format that is easily read by people but not by machines. The .melt() method is a handy tool for reshaping data into a useful form."],"metadata":{"id":"2lB1hTzyXQef"}},{"cell_type":"markdown","source":["# Using .melt() for stocks vs bond performance\n","It is widespread knowledge that the price of bonds is inversely related to the price of stocks. In this last exercise, you'll review many of the topics in this chapter to confirm this. You have been given a table of percent change of the US 10-year treasury bond price. It is in a wide format where there is a separate column for each year. You will need to use the .melt() method to reshape this table.\n","\n","Additionally, you will use the .query() method to filter out unneeded data. You will merge this table with a table of the percent change of the Dow Jones Industrial stock index price. Finally, you will plot data.\n","\n","The tables ten_yr and dji have been loaded for you.\n","\n","## Instructions\n","\n","Use .melt() on ten_yr to unpivot everything except the metric column, setting var_name='date' and value_name='close'. Save the result to bond_perc.\n","Using the .query() method, select only those rows were metric equals 'close', and save to bond_perc_close.\n","Use merge_ordered() to merge dji (left table) and bond_perc_close on date with an inner join, and set suffixes equal to ('_dow', '_bond'). Save the result to dow_bond.\n","Using dow_bond, plot only the Dow and bond values."],"metadata":{"id":"IveZzgklYT1R"}},{"cell_type":"code","source":["# Use melt on ten_yr, unpivot everything besides the metric column\n","bond_perc = ten_yr.melt(id_vars=\"metric\", var_name = \"date\", value_name = \"close\")\n","\n","# Use query on bond_perc to select only the rows where metric=close\n","bond_perc_close = bond_perc.query('metric==\"close\"')\n","\n","# Merge (ordered) dji and bond_perc_close on date with an inner join\n","dow_bond = pd.merge_ordered(dji, bond_perc_close, on=\"date\", how = \"inner\", suffixes =[\"_dow\", \"_bond\" ])\n","\n","# Plot only the close_dow and close_bond columns\n","dow_bond.plot(y = [\"close_dow\", \"close_bond\"], x='date', rot=90)\n","plt.show()"],"metadata":{"id":"S5TJVQ_1YcF4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Super job! You used many of the techniques we have reviewed in this chapter to produce the plot. The plot confirms that the bond and stock prices are inversely correlated. Often as the price of stocks increases, the price for bonds decreases."],"metadata":{"id":"YaAEXPrmYgIo"}}]}